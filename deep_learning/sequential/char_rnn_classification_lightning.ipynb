{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "96882891",
      "metadata": {
        "id": "96882891"
      },
      "source": [
        "*<small>Last updated: 2026-02-25 01:35:27 UTC | Student Version (No Solutions)</small>*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccusy7v71e",
      "metadata": {
        "id": "ccusy7v71e"
      },
      "source": [
        "**Student Version** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Instructor Version**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/mtgca/7d52f0b7b63c6317f0151fe1505d85c7/char_rnn_classification_lightning.ipynb) &nbsp;&nbsp;&nbsp; [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtgca/DL-Labs/blob/main/02%20Sequential%20Models/char_rnn_classification_lightning.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f304d90f",
      "metadata": {
        "id": "f304d90f"
      },
      "source": [
        "# üìî **Resoluci√≥n**\n",
        "Una vez que la nota sea entregada, encontrar√°s la resoluci√≥n en este link de notion:\n",
        "\n",
        "https://www.notion.so/R-Sequential-Models-30005737435780668286e83f53ce4780?source=copy_link\n",
        "\n",
        "Recuerda que existen varias formar de solucionar las actividades propuestas en los laboratorios. Toma la resoluci√≥n entregada como referencia. En caso de que el link de resoluci√≥n no est√© p√∫blico, solicita acceso en el mismo link de notion.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {
        "id": "0"
      },
      "source": [
        "# Character-Level RNN Classification with PyTorch Lightning\n",
        "\n",
        "This notebook reimplements the character-level RNN name classification task using PyTorch Lightning, following best practices including:\n",
        "- Proper train/val/test split\n",
        "- LightningModule and LightningDataModule patterns\n",
        "- Training metrics visualization\n",
        "- Reproducibility with random seeds\n",
        "\n",
        "Notebook based on https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b384516a",
      "metadata": {
        "id": "b384516a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b99e494",
      "metadata": {
        "id": "0b99e494"
      },
      "outputs": [],
      "source": [
        "# Instalar PyTorch Lightning si es necesario\n",
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d83ca677",
      "metadata": {
        "id": "d83ca677"
      },
      "source": [
        "# Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35569f0",
      "metadata": {
        "id": "d35569f0"
      },
      "outputs": [],
      "source": [
        "![ ! -f data.zip ] && wget https://download.pytorch.org/tutorial/data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f360ed9",
      "metadata": {
        "id": "2f360ed9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Check if data folder exists\n",
        "print(os.getcwd())\n",
        "if os.path.exists('./data'):\n",
        "    print('Data folder already exists. Using existing data.')\n",
        "    print('(If you need to re-extract, delete the data folder first)')\n",
        "else:\n",
        "    print('Unzipping data for the first time...')\n",
        "    with zipfile.ZipFile('data.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    os.remove('data.zip')\n",
        "    print('Done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1",
      "metadata": {
        "id": "1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from torchmetrics import Accuracy, F1Score\n",
        "\n",
        "import string\n",
        "import unicodedata\n",
        "import glob\n",
        "import os\n",
        "from io import open\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Enable Tensor Cores on CUDA devices for better performance\n",
        "torch.set_float32_matmul_precision('medium')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "pl.seed_everything(2024, workers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf9e4af2",
      "metadata": {
        "id": "bf9e4af2"
      },
      "source": [
        "# Performance Optimization Through Environment Detection\n",
        "The fundamental difference between VSCode remote containers and Google Colab lies in how they communicate with the user interface. In VSCode remote environments, your code executes inside a Docker container or remote server, and all output‚Äîincluding progress bar updates, print statements, and data transfers‚Äîmust traverse the network to reach your local VSCode client. When PyTorch Lightning's progress bar updates hundreds of times per epoch (once per batch), these frequent UI updates create massive network overhead, with each update requiring a round-trip between the container and your client. Similarly, spawning multiple data loader worker processes in a remote container creates severe inter-process communication (IPC) bottlenecks, as each worker must serialize and transfer data through multiple layers. In contrast, Google Colab runs in a browser with a local connection to its backend servers, making these operations far less costly.\n",
        "\n",
        "This environment detection code automatically identifies whether you're in a constrained remote environment and adjusts critical settings accordingly. For VSCode remote, it sets num_workers=0 to eliminate multi-process overhead (letting the GPU handle parallelism instead), disables progress_bar to eliminate hundreds of network round-trips per epoch, and reduces logging frequency by 10x. These changes transform training from 10-100x slower than native to near-native GPU speed‚Äîtypically resulting in 5-10x performance improvement. For Colab or local environments where these bottlenecks don't exist, it maintains standard settings with progress bars and parallel data loading. This automatic adaptation means you can run the same notebook efficiently in both environments without manual configuration changes, ensuring optimal performance regardless of where you execute your training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40aeabac",
      "metadata": {
        "id": "40aeabac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def is_remote_vscode():\n",
        "    \"\"\"Detect if running in VSCode remote container/SSH environment.\n",
        "\n",
        "    Uses DMI Product information to distinguish between VSCode Remote and Google Colab.\n",
        "    Google Colab runs on \"Google Compute Engine\" VMs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if os.path.exists('/sys/class/dmi/id/product_name'):\n",
        "            with open('/sys/class/dmi/id/product_name', 'r') as f:\n",
        "                product = f.read().strip()\n",
        "                print(\"DMI:\", product)\n",
        "                # Google Colab runs on Google Compute Engine\n",
        "                if 'Google' in product or 'Compute Engine' in product:\n",
        "                    return False  # This is Google Colab\n",
        "        # If not Google Compute Engine, assume VSCode Remote/Local\n",
        "        return True\n",
        "    except:\n",
        "        # If we can't read DMI, assume local/non-Colab\n",
        "        return True\n",
        "\n",
        "# Detect environment and set optimal configurations\n",
        "IS_REMOTE = is_remote_vscode()\n",
        "print(f\"Environment detected: {'VSCode Remote' if IS_REMOTE else 'Google Colab'}\")\n",
        "\n",
        "# Performance-optimized settings based on environment\n",
        "if IS_REMOTE:\n",
        "    NUM_WORKERS = 2  #  to avoid IPC overhead\n",
        "    PIN_MEMORY = False  # Reduce memory pressure in container\n",
        "    PERSISTENT_WORKERS = False\n",
        "    PREFETCH_FACTOR = None\n",
        "    ENABLE_PROGRESS_BAR = False  # Disable progress bar updates over network\n",
        "    LOG_EVERY_N_STEPS = 50  # Reduce logging frequency\n",
        "    CALLBACK_VERBOSE = True\n",
        "else:\n",
        "    NUM_WORKERS = 4  # Leverage multiple cores for data loading\n",
        "    PIN_MEMORY = True  # Faster GPU transfer\n",
        "    PERSISTENT_WORKERS = True  # Reuse worker processes\n",
        "    PREFETCH_FACTOR = 2  # Load batches ahead\n",
        "    ENABLE_PROGRESS_BAR = True\n",
        "    LOG_EVERY_N_STEPS = 10\n",
        "    CALLBACK_VERBOSE = True\n",
        "print(f\"DataLoader num_workers set to: {NUM_WORKERS}\")\n",
        "print(\"Progress bar set to\", ENABLE_PROGRESS_BAR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b2ef2a",
      "metadata": {
        "id": "01b2ef2a"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This code prepares text data for a character-level RNN by converting names into one-hot encoded tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9cc8d0c",
      "metadata": {
        "id": "e9cc8d0c"
      },
      "outputs": [],
      "source": [
        "# Character set and conversion functions\n",
        "#Defines the vocabulary: a-z, A-Z, space, punctuation, and \"_\" for unknown characters.\n",
        "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
        "n_letters = len(allowed_characters)   # 58 characters total\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    \"\"\"Convert Unicode string to plain ASCII.\n",
        "    Normalizes Unicode text to ASCII (e.g., \"≈ölus√†rski\" ‚Üí \"Slusarski\"):\n",
        "    normalize('NFD'): Decomposes accented characters (√© ‚Üí e + ¬¥)\n",
        "    Filters out accent marks (category 'Mn')\n",
        "    Keeps only allowed characters\n",
        "    \"\"\"\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in allowed_characters\n",
        "    )\n",
        "\n",
        "def letterToIndex(letter):\n",
        "    \"\"\"Find letter index from allowed_characters.\n",
        "    Maps each character to its position in the vocabulary:\n",
        "    'a' ‚Üí 0, 'b' ‚Üí 1, etc.\n",
        "    Unknown characters ‚Üí index of \"_\"\n",
        "    \"\"\"\n",
        "    if letter not in allowed_characters:\n",
        "        return allowed_characters.find(\"_\")\n",
        "    else:\n",
        "        return allowed_characters.find(letter)\n",
        "\n",
        "def lineToTensor(line):\n",
        "    \"\"\"Turn a line into a <line_length x 1 x n_letters> tensor.\n",
        "    Converts a string into a one-hot encoded tensor of shape [length, 1, 58]:\n",
        "    Each character becomes a vector with all zeros except a 1 at its index position\n",
        "    \"Ab\" ‚Üí [[1,0,0,...], [0,1,0,...]] (2 timesteps, each with 58-dim vector)\n",
        "    The middle dimension (1) represents batch size\n",
        "    \"\"\"\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "print(f\"Number of characters: {n_letters}\")\n",
        "print(f\"Example conversion: '≈ölus√†rski' -> '{unicodeToAscii('≈ölus√†rski')}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4",
      "metadata": {
        "id": "4"
      },
      "outputs": [],
      "source": [
        "class NamesDataset(Dataset):\n",
        "    \"\"\"Dataset for name classification.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        labels_set = set()\n",
        "\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Read all .txt files in the specified directory\n",
        "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
        "        for filename in sorted(text_files):  # Sort for reproducibility\n",
        "            label = os.path.splitext(os.path.basename(filename))[0]\n",
        "            labels_set.add(label)\n",
        "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "            for name in lines:\n",
        "                self.data.append(unicodeToAscii(name))\n",
        "                self.labels.append(label)\n",
        "\n",
        "        # Create label to index mapping\n",
        "        self.labels_uniq = sorted(list(labels_set))\n",
        "        self.label_to_idx = {label: idx for idx, label in enumerate(self.labels_uniq)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert to tensors (one hot encoding)\n",
        "        #Ana ->one-hot encoding for each char\n",
        "        name_tensor = lineToTensor(name)\n",
        "        label_idx = self.label_to_idx[label]\n",
        "\n",
        "        return name_tensor, label_idx\n",
        "\n",
        "    @property\n",
        "    def num_classes(self):\n",
        "        return len(self.labels_uniq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5",
      "metadata": {
        "id": "5"
      },
      "source": [
        "## Lightning Data Module\n",
        "\n",
        "We'll create a LightningDataModule to handle data loading with train/val/test splits.\n",
        "\n",
        "collate_fn is called by PyTorch's DataLoader to combine multiple samples into a single batch:\n",
        "\n",
        "1. Gets variable-length sequences: \"Ann\" (3 chars), \"Alexander\" (9 chars), \"Li\" (2 chars)\n",
        "\n",
        "2. Pads to max length: All sequences padded to length 9:\n",
        "\n",
        "\"Ann\" ‚Üí 3 real chars + 6 padding zeros\n",
        "\n",
        "\"Alexander\" ‚Üí 9 real chars\n",
        "\n",
        "\"Li\" ‚Üí 2 real chars + 7 padding zeros\n",
        "\n",
        "3. Tracks original lengths: [3, 9, 2] - needed so the RNN knows which values are padding\n",
        "\n",
        "4. Returns batch-ready tensors:\n",
        "\n",
        "padded_names: shape [9, 3, 58] (max_len, batch_size, features)\n",
        "\n",
        "labels_tensor: shape [3]\n",
        "\n",
        "lengths: [3, 9, 2]\n",
        "\n",
        "With padding + batch processing, the RNN can process 3 (or 64) names simultaneously instead of one-at-a-time, achieving massive speedup on GPUs while still respecting the true length of each sequence (using pack_padded_sequence in the forward pass)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6",
      "metadata": {
        "id": "6"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to handle variable length sequences with padding.\n",
        "\n",
        "    Pads sequences to the same length and returns lengths for pack_padded_sequence.\n",
        "    \"\"\"\n",
        "    names, labels = zip(*batch)\n",
        "\n",
        "    # Get sequence lengths (before padding)\n",
        "    lengths = torch.tensor([name.size(0) for name in names], dtype=torch.long)\n",
        "\n",
        "    # Pad sequences: pad_sequence expects (seq_len, batch, features)\n",
        "    # Our tensors are (seq_len, 1, 58), so we need to squeeze and then pad\n",
        "    names_squeezed = [name.squeeze(1) for name in names]  # Remove middle dimension\n",
        "    padded_names = pad_sequence(names_squeezed, batch_first=False, padding_value=0.0)\n",
        "    # Result shape: (max_seq_len, batch_size, 58)\n",
        "\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return padded_names, labels_tensor, lengths\n",
        "\n",
        "\n",
        "class NamesDataModule(pl.LightningDataModule):\n",
        "    \"\"\"Lightning DataModule for names dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir: str, batch_size: int = 32, num_workers: int = 0,\n",
        "                 train_ratio: float = 0.7, val_ratio: float = 0.15, test_ratio: float = 0.15):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.train_ratio = train_ratio\n",
        "        self.val_ratio = val_ratio\n",
        "        self.test_ratio = test_ratio\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"Load and split dataset.\"\"\"\n",
        "        # Load full dataset\n",
        "        full_dataset = NamesDataset(self.data_dir)\n",
        "        self.labels_uniq = full_dataset.labels_uniq\n",
        "        self.num_classes = full_dataset.num_classes\n",
        "\n",
        "        # Calculate split sizes\n",
        "        total_size = len(full_dataset)\n",
        "        train_size = int(self.train_ratio * total_size)\n",
        "        val_size = int(self.val_ratio * total_size)\n",
        "        test_size = total_size - train_size - val_size\n",
        "\n",
        "        # Split dataset\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
        "            full_dataset,\n",
        "            [train_size, val_size, test_size],\n",
        "            generator=torch.Generator().manual_seed(2024)\n",
        "        )\n",
        "\n",
        "        print(f\"Dataset split: Train={len(self.train_dataset)}, \"\n",
        "              f\"Val={len(self.val_dataset)}, Test={len(self.test_dataset)}\")\n",
        "        print(f\"Number of classes: {self.num_classes}\")\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=PIN_MEMORY,\n",
        "            persistent_workers=True if self.num_workers > 0 else False\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=PIN_MEMORY,\n",
        "            persistent_workers=True if self.num_workers > 0 else False\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(\n",
        "            self.test_dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=PIN_MEMORY,\n",
        "            persistent_workers=True if self.num_workers > 0 else False\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7",
      "metadata": {
        "id": "7"
      },
      "source": [
        "## Lightning Module\n",
        "\n",
        "We'll implement the CharRNN model as a LightningModule with training, validation, and test steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b547bdae",
      "metadata": {
        "id": "b547bdae"
      },
      "source": [
        "### Understanding RNN Outputs: Why We Use `hidden[0]` Instead of Last Output\n",
        "\n",
        "When working with RNNs for sequence classification, a common question arises: **Why do we use the hidden state (`hidden[0]`) instead of extracting the last output from the RNN?**\n",
        "\n",
        "This is an important concept to understand, especially when dealing with variable-length sequences and padding. Let's break it down step by step.\n",
        "\n",
        "#### What Does an RNN Return?\n",
        "\n",
        "When you call an RNN in PyTorch, it returns two things:\n",
        "\n",
        "```python\n",
        "packed_output, hidden = self.rnn(packed_input)\n",
        "```\n",
        "\n",
        "1. **`packed_output`**: A `PackedSequence` containing the RNN's output at **every timestep**\n",
        "   - When unpacked, shape: `(seq_len, batch_size, hidden_size)`\n",
        "   - Contains the output computed at each position in the sequence\n",
        "   - For packed sequences, only contains values for non-padded positions\n",
        "\n",
        "2. **`hidden`**: The **final hidden state** tensor\n",
        "   - Shape: `(num_layers, batch_size, hidden_size)`\n",
        "   - For our single-layer RNN: `(1, batch_size, hidden_size)`\n",
        "   - Contains the hidden state after processing the entire sequence\n",
        "\n",
        "#### The Challenge with Variable-Length Sequences\n",
        "\n",
        "Consider a batch with three names of different lengths:\n",
        "- **Sequence 1**: \"Bob\" (length 3)\n",
        "- **Sequence 2**: \"Alexandra\" (length 9)\n",
        "- **Sequence 3**: \"Li\" (length 2)\n",
        "\n",
        "After padding all sequences to the maximum length (9 characters), our padded batch looks like this:\n",
        "\n",
        "```\n",
        "Position:  0  1  2  3  4  5  6  7  8\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Seq 1:     B  o  b  [P][P][P][P][P][P]    [P] = padding\n",
        "Seq 2:     A  l  e  x  a  n  d  r  a\n",
        "Seq 3:     L  i  [P][P][P][P][P][P][P]\n",
        "```\n",
        "\n",
        "Now, here's the critical question: **Where is the \"last output\" for each sequence?**\n",
        "\n",
        "- For \"Bob\": The last **real** output is at position 2 (after \"b\")\n",
        "- For \"Alexandra\": The last real output is at position 8 (after \"a\")\n",
        "- For \"Li\": The last real output is at position 1 (after \"i\")\n",
        "\n",
        "The \"last output\" is at **different positions** for different sequences!\n",
        "\n",
        "#### Why We Can't Simply Use `packed_output[-1]`\n",
        "\n",
        "If we naively tried to extract the last output like this:\n",
        "\n",
        "```python\n",
        "# WRONG APPROACH!\n",
        "last_output = packed_output[-1]  # Gets output at position 8 for ALL sequences\n",
        "```\n",
        "\n",
        "This would give us:\n",
        "- For \"Bob\": Output at position 8 ‚Üí **padding garbage!** ‚ùå\n",
        "- For \"Alexandra\": Output at position 8 ‚Üí correct final output ‚úì\n",
        "- For \"Li\": Output at position 8 ‚Üí **padding garbage!** ‚ùå\n",
        "\n",
        "This doesn't work! We'd need to manually index each sequence by its actual length:\n",
        "\n",
        "```python\n",
        "# Inefficient and error-prone approach\n",
        "for i in range(batch_size):\n",
        "    last_output_i = packed_output[lengths[i]-1, i, :]  # Manual indexing by length\n",
        "```\n",
        "\n",
        "This defeats the purpose of batching and is cumbersome.\n",
        "\n",
        "#### The Elegant Solution: Using `hidden[0]`\n",
        "\n",
        "PyTorch's RNN has a clever solution: the `hidden` tensor **automatically contains the final hidden state** for each sequence, correctly handling variable lengths when you use `pack_padded_sequence`.\n",
        "\n",
        "```python\n",
        "hidden[0]  # Shape: (batch_size, hidden_size)\n",
        "```\n",
        "\n",
        "What does `hidden[0]` contain?\n",
        "- `hidden[0][0]` ‚Üí Final hidden state after processing \"b\" (position 2 for Seq 1) ‚úì\n",
        "- `hidden[0][1]` ‚Üí Final hidden state after processing \"a\" (position 8 for Seq 2) ‚úì\n",
        "- `hidden[0][2]` ‚Üí Final hidden state after processing \"i\" (position 1 for Seq 3) ‚úì\n",
        "\n",
        "**Perfect!** Each row contains the correct final state, regardless of padding.\n",
        "\n",
        "#### But Wait‚ÄîAren't They the Same Thing?\n",
        "\n",
        "You might wonder: \"Isn't the last output the same as the final hidden state?\"\n",
        "\n",
        "**Conceptually, yes!** For a single-layer RNN, the output at timestep `t` is derived from the hidden state at timestep `t`. At the last real timestep of a sequence, they represent the same information.\n",
        "\n",
        "**Practically, no!** The key differences are:\n",
        "\n",
        "1. **Location**:\n",
        "   - Last outputs are scattered at different indices in `packed_output`\n",
        "   - Final hidden states are neatly collected in `hidden[0]`\n",
        "\n",
        "2. **Convenience**:\n",
        "   - `hidden[0]` gives you shape `(batch_size, hidden_size)` directly\n",
        "   - Extracting from `packed_output` requires manual indexing by sequence lengths\n",
        "\n",
        "3. **Efficiency**:\n",
        "   - Using `hidden[0]` is a single tensor slice\n",
        "   - Manual extraction requires a loop or complex indexing\n",
        "\n",
        "4. **Best Practice**:\n",
        "   - `hidden[0]` is the idiomatic PyTorch pattern for sequence classification\n",
        "   - It's what you'll see in tutorials, papers, and production code\n",
        "\n",
        "#### Summary\n",
        "\n",
        "When we write:\n",
        "```python\n",
        "output = self.h2o(hidden[0])\n",
        "```\n",
        "\n",
        "We're using `hidden[0]` because:\n",
        "- ‚úÖ It automatically contains the final state after each sequence's last real character\n",
        "- ‚úÖ It handles variable-length sequences correctly with `pack_padded_sequence`\n",
        "- ‚úÖ It's efficient and doesn't require manual indexing\n",
        "- ‚úÖ It's the standard PyTorch pattern for sequence classification\n",
        "\n",
        "This is exactly what we want for classifying names‚Äîthe network's final representation of each sequence, ready to be transformed into class predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "id": "8"
      },
      "outputs": [],
      "source": [
        "class CharRNNLightning(pl.LightningModule):\n",
        "    \"\"\"Character-level RNN for name classification using PyTorch Lightning.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.005):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=False)\n",
        "        self.h2o = nn.Linear(hidden_size, output_size)\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Metrics\n",
        "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=output_size)\n",
        "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=output_size)\n",
        "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=output_size)\n",
        "\n",
        "        self.train_f1 = F1Score(task=\"multiclass\", num_classes=output_size, average='weighted')\n",
        "        self.val_f1 = F1Score(task=\"multiclass\", num_classes=output_size, average='weighted')\n",
        "        self.test_f1 = F1Score(task=\"multiclass\", num_classes=output_size, average='weighted')\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Padded input tensor of shape (seq_len, batch_size, input_size)\n",
        "            lengths: Tensor of sequence lengths for pack_padded_sequence\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # Pack padded sequences for efficient RNN processing\n",
        "        packed_input = pack_padded_sequence(x, lengths.cpu(), enforce_sorted=False)\n",
        "\n",
        "        # Run through RNN\n",
        "        packed_output, hidden = self.rnn(packed_input)\n",
        "\n",
        "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
        "        # We use hidden[0] which is shape (batch_size, hidden_size)\n",
        "        output = self.h2o(hidden[0])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"Training step.\"\"\"\n",
        "        names, labels, lengths = batch\n",
        "\n",
        "        # Single forward pass for entire batch\n",
        "        output = self(names, lengths)\n",
        "        loss = self.criterion(output, labels)\n",
        "\n",
        "        # Compute metrics using torchmetrics\n",
        "        preds = output.argmax(dim=1)\n",
        "        self.train_acc(preds, labels)\n",
        "        self.train_f1(preds, labels)\n",
        "\n",
        "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=len(labels))\n",
        "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('train_f1', self.train_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Validation step.\"\"\"\n",
        "        names, labels, lengths = batch\n",
        "\n",
        "        # Single forward pass for entire batch\n",
        "        output = self(names, lengths)\n",
        "        loss = self.criterion(output, labels)\n",
        "\n",
        "        # Compute metrics using torchmetrics\n",
        "        preds = output.argmax(dim=1)\n",
        "        self.val_acc(preds, labels)\n",
        "        self.val_f1(preds, labels)\n",
        "\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=len(labels))\n",
        "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"Test step.\"\"\"\n",
        "        names, labels, lengths = batch\n",
        "\n",
        "        # Single forward pass for entire batch\n",
        "        output = self(names, lengths)\n",
        "        loss = self.criterion(output, labels)\n",
        "\n",
        "        # Compute metrics using torchmetrics\n",
        "        preds = output.argmax(dim=1)\n",
        "        self.test_acc(preds, labels)\n",
        "        self.test_f1(preds, labels)\n",
        "\n",
        "        self.log('test_loss', loss, on_step=False, on_epoch=True, batch_size=len(labels))\n",
        "        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True)\n",
        "        self.log('test_f1', self.test_f1, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Configure optimizer.\"\"\"\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'monitor': 'val_loss',\n",
        "                'interval': 'epoch',\n",
        "                'frequency': 1\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fo3u1icxrzg",
      "metadata": {
        "id": "0fo3u1icxrzg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "class SimpleEpochProgressCallback(pl.Callback):\n",
        "    \"\"\"Lightweight callback to show epoch progress without progress bar overhead.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epoch_start_time = None\n",
        "\n",
        "    def on_train_start(self, trainer, pl_module):\n",
        "        \"\"\"Print training configuration at the start.\"\"\"\n",
        "        train_dataloader = trainer.train_dataloader\n",
        "\n",
        "        # Get batch size and number of batches\n",
        "        if hasattr(train_dataloader, 'batch_size'):\n",
        "            batch_size = train_dataloader.batch_size\n",
        "        elif hasattr(train_dataloader, 'loaders') and hasattr(train_dataloader.loaders, 'batch_size'):\n",
        "            batch_size = train_dataloader.loaders.batch_size\n",
        "        else:\n",
        "            batch_size = getattr(train_dataloader, 'batch_size', 'N/A')\n",
        "\n",
        "        num_batches = len(train_dataloader)\n",
        "\n",
        "        print(f\"\\nTraining Configuration: Batch Size={batch_size}, Steps per Epoch={num_batches}\\n\")\n",
        "\n",
        "    def on_train_epoch_start(self, trainer, pl_module):\n",
        "        \"\"\"Record the start time of the epoch.\"\"\"\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        \"\"\"Print single-line summary at the end of each training epoch.\"\"\"\n",
        "        current_epoch = trainer.current_epoch + 1\n",
        "        max_epochs = trainer.max_epochs\n",
        "\n",
        "        # Calculate epoch duration\n",
        "        epoch_time = time.time() - self.epoch_start_time if self.epoch_start_time else 0\n",
        "\n",
        "        # Get metrics from logged values\n",
        "        train_loss = trainer.callback_metrics.get('train_loss', None)\n",
        "        val_loss = trainer.callback_metrics.get('val_loss', None)\n",
        "        val_acc = trainer.callback_metrics.get('val_acc', None)\n",
        "\n",
        "        # Build single-line output\n",
        "        parts = [f\"Epoch {current_epoch}/{max_epochs}\"]\n",
        "\n",
        "        if train_loss is not None:\n",
        "            parts.append(f\"Train Loss: {train_loss:.4f}\")\n",
        "        if val_loss is not None:\n",
        "            parts.append(f\"Val Loss: {val_loss:.4f}\")\n",
        "        if val_acc is not None:\n",
        "            parts.append(f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        parts.append(f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        print(\" | \".join(parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {
        "id": "9"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now we'll set up the trainer with callbacks and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "id": "10"
      },
      "outputs": [],
      "source": [
        "# Initialize data module\n",
        "data_module = NamesDataModule(\n",
        "    data_dir='data/names',\n",
        "    batch_size=32,\n",
        "    train_ratio=0.7,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Setup data\n",
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11",
      "metadata": {
        "id": "11"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = CharRNNLightning(\n",
        "    input_size=n_letters,\n",
        "    hidden_size=128,\n",
        "    output_size=data_module.num_classes,\n",
        "    learning_rate=0.005\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ahgx9oj2mca",
      "metadata": {
        "id": "ahgx9oj2mca"
      },
      "source": [
        "## üî¢ Understanding RNN Parameter Calculation\n",
        "\n",
        "When you see `RNN(58, 128)` in the model architecture above, you might wonder: **How many parameters does this RNN layer actually have?**\n",
        "\n",
        "### The Four Components of RNN Parameters\n",
        "\n",
        "A vanilla RNN layer contains **four sets of learnable parameters**:\n",
        "\n",
        "1. **Input-to-Hidden Weights (W_ih)**: `input_size √ó hidden_size`\n",
        "   - Transforms the input at each timestep to hidden state dimension\n",
        "   - For our model: `58 √ó 128 = 7,424` parameters\n",
        "\n",
        "2. **Hidden-to-Hidden Weights (W_hh)**: `hidden_size √ó hidden_size`\n",
        "   - Transforms the previous hidden state to the current hidden state\n",
        "   - For our model: `128 √ó 128 = 16,384` parameters\n",
        "\n",
        "3. **Input-to-Hidden Bias (b_ih)**: `hidden_size`\n",
        "   - Bias term for the input transformation\n",
        "   - For our model: `128` parameters\n",
        "\n",
        "4. **Hidden-to-Hidden Bias (b_hh)**: `hidden_size`\n",
        "   - Bias term for the hidden state transformation\n",
        "   - For our model: `128` parameters\n",
        "\n",
        "### The Formula\n",
        "\n",
        "**Total RNN parameters = (input_size √ó hidden_size) + (hidden_size √ó hidden_size) + (2 √ó hidden_size)**\n",
        "\n",
        "Or more compactly:\n",
        "\n",
        "**Total RNN parameters = hidden_size √ó (input_size + hidden_size + 2)**\n",
        "\n",
        "### Calculation for This Model\n",
        "\n",
        "```\n",
        "RNN layer parameters = 128 √ó (58 + 128 + 2)\n",
        "                     = 128 √ó 188\n",
        "                     = 24,064 parameters\n",
        "```\n",
        "\n",
        "### Complete Model Parameters\n",
        "\n",
        "Our complete model has two components:\n",
        "\n",
        "| Component | Calculation | Parameters |\n",
        "|-----------|-------------|------------|\n",
        "| **RNN layer** | `128 √ó (58 + 128 + 2)` | **24,064** |\n",
        "| **Linear layer (h2o)** | `(128 √ó 18) + 18` | **2,322** |\n",
        "| **Total** | | **26,386** |\n",
        "\n",
        "The linear layer has:\n",
        "- Weight matrix: `128 √ó 18 = 2,304` parameters\n",
        "- Bias vector: `18` parameters\n",
        "\n",
        "### The Math Behind It\n",
        "\n",
        "The RNN computes the hidden state at each timestep using this formula:\n",
        "\n",
        "```\n",
        "h_t = tanh(W_ih @ x_t + b_ih + W_hh @ h_{t-1} + b_hh)\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `x_t` is the input at timestep t (shape: `[58]`)\n",
        "- `h_{t-1}` is the previous hidden state (shape: `[128]`)\n",
        "- `h_t` is the current hidden state (shape: `[128]`)\n",
        "- `W_ih` is `[128, 58]` ‚Üí 7,424 parameters\n",
        "- `W_hh` is `[128, 128]` ‚Üí 16,384 parameters\n",
        "- `b_ih` is `[128]` ‚Üí 128 parameters\n",
        "- `b_hh` is `[128]` ‚Üí 128 parameters\n",
        "\n",
        "This gives us **24,064 parameters for just the RNN layer**.\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "Understanding parameter counts helps you:\n",
        "1. **Compare architectures**: LSTM has 4√ó more parameters than RNN for the same hidden size\n",
        "2. **Estimate memory**: Each parameter typically requires 4 bytes (float32)\n",
        "3. **Prevent overfitting**: More parameters need more training data\n",
        "4. **Debug models**: Unexpected parameter counts can reveal architecture mistakes\n",
        "\n",
        "For example, in this notebook:\n",
        "- **Vanilla RNN**: 24,064 parameters (efficient but simple)\n",
        "- **LSTM**: ~74K parameters (4√ó more, better at long sequences)\n",
        "- **GRU**: ~56K parameters (3√ó more, good balance)\n",
        "\n",
        "The RNN's simplicity (fewer parameters) makes it fast to train but limits its ability to capture complex temporal dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1d675a",
      "metadata": {
        "id": "db1d675a"
      },
      "source": [
        "### Quick Verification\n",
        "\n",
        "You can verify this calculation with PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8f39dc",
      "metadata": {
        "id": "bb8f39dc"
      },
      "outputs": [],
      "source": [
        "# Count only RNN parameters\n",
        "rnn_params = sum(p.numel() for p in model.rnn.parameters())\n",
        "print(f\"RNN parameters: {rnn_params:,}\")  # Output: 24,064\n",
        "\n",
        "# Count only Linear layer parameters\n",
        "linear_params = sum(p.numel() for p in model.h2o.parameters())\n",
        "print(f\"Linear parameters: {linear_params:,}\")  # Output: 2,322\n",
        "\n",
        "# Total model parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")  # Output: 26,386"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {
        "id": "12"
      },
      "outputs": [],
      "source": [
        "# Configure callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath='checkpoints/',\n",
        "    filename='char-rnn-{epoch:02d}-{val_loss:.2f}',\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_top_k=3,\n",
        "    verbose=CALLBACK_VERBOSE\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    mode='min',\n",
        "    verbose=CALLBACK_VERBOSE\n",
        ")\n",
        "\n",
        "# CSV logger for easy plotting\n",
        "csv_logger = CSVLogger('logs/', name='char_rnn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {
        "id": "13"
      },
      "outputs": [],
      "source": [
        "# Add epoch progress callback to callbacks list\n",
        "epoch_progress = SimpleEpochProgressCallback()\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=50,\n",
        "    callbacks=[checkpoint_callback, early_stopping, epoch_progress],\n",
        "    logger=csv_logger,\n",
        "    deterministic=True,\n",
        "    log_every_n_steps=LOG_EVERY_N_STEPS,\n",
        "    enable_progress_bar=ENABLE_PROGRESS_BAR\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(model, data_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {
        "id": "14"
      },
      "source": [
        "## Plotting Training Curves\n",
        "\n",
        "Let's visualize the training and validation loss over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15",
      "metadata": {
        "id": "15"
      },
      "outputs": [],
      "source": [
        "# Load metrics from CSV logger\n",
        "metrics = pd.read_csv(f'{csv_logger.log_dir}/metrics.csv')\n",
        "\n",
        "# Aggregate by epoch (remove NaN values)\n",
        "train_loss = metrics[['epoch', 'train_loss']].dropna()\n",
        "val_loss = metrics[['epoch', 'val_loss']].dropna()\n",
        "val_acc = metrics[['epoch', 'val_acc']].dropna()\n",
        "\n",
        "# Create figure with subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot loss\n",
        "ax1.plot(train_loss['epoch'], train_loss['train_loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(val_loss['epoch'], val_loss['val_loss'], label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "ax2.plot(val_acc['epoch'], val_acc['val_acc'], label='Val Accuracy', marker='s', color='green')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Training Loss: {train_loss['train_loss'].iloc[-1]:.4f}\")\n",
        "print(f\"Final Validation Loss: {val_loss['val_loss'].iloc[-1]:.4f}\")\n",
        "print(f\"Final Validation Accuracy: {val_acc['val_acc'].iloc[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "id": "16"
      },
      "source": [
        "## Test Evaluation\n",
        "\n",
        "Finally, let's evaluate the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17",
      "metadata": {
        "id": "17"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "test_results = trainer.test(model, data_module, ckpt_path='best')\n",
        "\n",
        "print(f\"\\nTest Results:\")\n",
        "print(f\"Test Loss: {test_results[0]['test_loss']:.4f}\")\n",
        "print(f\"Test Accuracy: {test_results[0]['test_acc']:.4f}\")\n",
        "print(f\"Test F1 Score: {test_results[0]['test_f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u00a5fbjunl",
      "metadata": {
        "id": "u00a5fbjunl"
      },
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "Let's visualize the confusion matrix on the test set to see which languages the model confuses with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x9tfuez7wm8",
      "metadata": {
        "id": "x9tfuez7wm8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def compute_confusion_matrix(model, dataloader, labels):\n",
        "    \"\"\"Compute confusion matrix for the given dataloader.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            names, labels_batch, lengths = batch\n",
        "\n",
        "            # Process entire batch at once\n",
        "            output = model(names, lengths)\n",
        "            preds = output.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "    return confusion_matrix(all_labels, all_preds), all_preds, all_labels\n",
        "\n",
        "# Compute confusion matrix on test set\n",
        "conf_matrix, predictions, true_labels = compute_confusion_matrix(\n",
        "    model,\n",
        "    data_module.test_dataloader(),\n",
        "    data_module.labels_uniq\n",
        ")\n",
        "\n",
        "# Normalize by row (true labels)\n",
        "conf_matrix_normalized = conf_matrix.astype('float')\n",
        "for i in range(len(data_module.labels_uniq)):\n",
        "    row_sum = conf_matrix_normalized[i].sum()\n",
        "    if row_sum > 0:\n",
        "        conf_matrix_normalized[i] = conf_matrix_normalized[i] / row_sum\n",
        "\n",
        "# Plot confusion matrix\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Plot raw counts\n",
        "im1 = ax1.matshow(conf_matrix, cmap='Blues')\n",
        "ax1.set_title('Confusion Matrix (Counts)', pad=20, fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
        "ax1.set_ylabel('True Label', fontsize=12)\n",
        "ax1.set_xticks(np.arange(len(data_module.labels_uniq)))\n",
        "ax1.set_yticks(np.arange(len(data_module.labels_uniq)))\n",
        "ax1.set_xticklabels(data_module.labels_uniq, rotation=90)\n",
        "ax1.set_yticklabels(data_module.labels_uniq)\n",
        "ax1.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax1.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
        "\n",
        "# Plot normalized (percentages)\n",
        "im2 = ax2.matshow(conf_matrix_normalized, cmap='Blues', vmin=0, vmax=1)\n",
        "ax2.set_title('Confusion Matrix (Normalized)', pad=20, fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
        "ax2.set_ylabel('True Label', fontsize=12)\n",
        "ax2.set_xticks(np.arange(len(data_module.labels_uniq)))\n",
        "ax2.set_yticks(np.arange(len(data_module.labels_uniq)))\n",
        "ax2.set_xticklabels(data_module.labels_uniq, rotation=90)\n",
        "ax2.set_yticklabels(data_module.labels_uniq)\n",
        "ax2.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "ax2.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "plt.colorbar(im2, ax=ax2, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display per-class accuracy\n",
        "print(\"\\nPer-class Accuracy on Test Set:\")\n",
        "print(\"-\" * 50)\n",
        "for i, label in enumerate(data_module.labels_uniq):\n",
        "    accuracy = conf_matrix_normalized[i, i]\n",
        "    print(f\"{label:15s}: {accuracy:.2%} ({conf_matrix[i, i]}/{conf_matrix[i].sum()})\")\n",
        "\n",
        "# Find most common confusions\n",
        "print(\"\\nMost Common Confusions (excluding correct predictions):\")\n",
        "print(\"-\" * 50)\n",
        "confusions = []\n",
        "for i in range(len(data_module.labels_uniq)):\n",
        "    for j in range(len(data_module.labels_uniq)):\n",
        "        if i != j and conf_matrix[i, j] > 0:\n",
        "            confusions.append((\n",
        "                data_module.labels_uniq[i],\n",
        "                data_module.labels_uniq[j],\n",
        "                conf_matrix[i, j],\n",
        "                conf_matrix_normalized[i, j]\n",
        "            ))\n",
        "\n",
        "# Sort by count and show top 10\n",
        "confusions.sort(key=lambda x: x[2], reverse=True)\n",
        "for true_label, pred_label, count, percentage in confusions[:10]:\n",
        "    print(f\"{true_label:15s} ‚Üí {pred_label:15s}: {count:4d} times ({percentage:.1%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {
        "id": "18"
      },
      "source": [
        "## Inference Example\n",
        "\n",
        "Let's test the model with some example names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19",
      "metadata": {
        "id": "19"
      },
      "outputs": [],
      "source": [
        "def predict_name(model, name, labels):\n",
        "    \"\"\"Predict the language of origin for a given name.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        name_tensor = lineToTensor(name)#.squeeze(1)  # Remove batch dim: (seq_len, 58)\n",
        "       # name_tensor = name_tensor.unsqueeze(1)  # Add batch dim: (seq_len, 1, 58)\n",
        "        lengths = torch.tensor([name_tensor.size(0)], dtype=torch.long)\n",
        "\n",
        "        output = model(name_tensor, lengths)\n",
        "        probabilities = F.softmax(output, dim=1)\n",
        "        top_prob, top_idx = probabilities.topk(3)\n",
        "\n",
        "        print(f\"\\nName: {name}\")\n",
        "        print(\"Top 3 predictions:\")\n",
        "        for i in range(3):\n",
        "            label = labels[top_idx[0][i].item()]\n",
        "            prob = top_prob[0][i].item()\n",
        "            print(f\"  {i+1}. {label}: {prob:.2%}\")\n",
        "\n",
        "# Test with example names\n",
        "test_names = ['Rodriguez', 'Smith', 'Wang', 'O\\'Brien', 'Kowalski', 'Nakamura']\n",
        "\n",
        "for name in test_names:\n",
        "    predict_name(model, name, data_module.labels_uniq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {
        "id": "20"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. **PyTorch Lightning best practices**: Using LightningModule and LightningDataModule for clean, organized code\n",
        "2. **Proper data splitting**: 70% train, 15% validation, 15% test\n",
        "3. **Training monitoring**: Logging train and validation metrics, with plots\n",
        "4. **Callbacks**: Model checkpointing and early stopping\n",
        "5. **Reproducibility**: Fixed random seeds throughout\n",
        "6. **Learning rate scheduling**: ReduceLROnPlateau for adaptive learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tk4415cs8f",
      "metadata": {
        "id": "tk4415cs8f"
      },
      "source": [
        "## üéØ Activity 1: Implement LSTM for Name Classification\n",
        "\n",
        "### Objective\n",
        "Modify the character-level RNN model to use **LSTM (Long Short-Term Memory)** architecture and compare its performance with the vanilla RNN.\n",
        "\n",
        "### Background\n",
        "LSTM is an advanced recurrent architecture that addresses the vanishing gradient problem in vanilla RNNs. It uses three gates to control information flow:\n",
        "- **Input gate**: Controls what new information to add to the cell state\n",
        "- **Forget gate**: Controls what information to discard from the cell state\n",
        "- **Output gate**: Controls what information to output from the cell state\n",
        "\n",
        "This gating mechanism allows LSTMs to capture long-term dependencies more effectively than vanilla RNNs.\n",
        "\n",
        "### Your Task\n",
        "Create a new model class `CharLSTMLightning` that uses `nn.LSTM` instead of `nn.RNN` to classify names by language of origin.\n",
        "\n",
        "### Key Differences to Handle\n",
        "1. **Module replacement**: Replace `nn.RNN` with `nn.LSTM` in your model definition\n",
        "2. **Output format**: LSTM returns `(output, (hidden, cell))` while RNN returns `(output, hidden)`\n",
        "   - You'll need to unpack the tuple: `packed_output, (hidden, cell) = self.lstm(packed_input)`\n",
        "3. **Final state extraction**: After unpacking, use `hidden[0]` to get the final hidden state for classification\n",
        "\n",
        "### Hints\n",
        "- Start by copying the `CharRNNLightning` class and renaming it to `CharLSTMLightning`\n",
        "- Change `self.rnn` to `self.lstm` and use `nn.LSTM` instead of `nn.LSTM`\n",
        "- In the forward method, handle the LSTM's tuple output: `(hidden, cell)`\n",
        "- Everything else remains the same!\n",
        "\n",
        "### Expected Outcome\n",
        "After training, you should achieve similar or better test accuracy compared to the vanilla RNN (~79-82%). LSTMs often perform better on sequence tasks due to their ability to maintain long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc54054",
      "metadata": {
        "id": "acc54054"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "srsojxt50qi",
      "metadata": {
        "id": "srsojxt50qi"
      },
      "source": [
        "## üéØ Activity 2: Implement GRU for Name Classification\n",
        "\n",
        "### Objective\n",
        "Modify the character-level RNN model to use **GRU (Gated Recurrent Unit)** architecture and compare its performance with both vanilla RNN and LSTM.\n",
        "\n",
        "### Background\n",
        "GRU is a recurrent architecture introduced as a simpler alternative to LSTM. It uses only two gates instead of three:\n",
        "- **Reset gate**: Controls how much of the previous hidden state to forget\n",
        "- **Update gate**: Controls how much of the new content to add to the hidden state\n",
        "\n",
        "GRU combines the forget and input gates of LSTM into a single update gate, making it computationally more efficient while maintaining similar performance. GRUs have fewer parameters than LSTMs, which can lead to faster training and less overfitting on smaller datasets.\n",
        "\n",
        "### Your Task\n",
        "Create a new model class `CharGRULightning` that uses `nn.GRU` instead of `nn.RNN` to classify names by language of origin.\n",
        "\n",
        "### Key Differences to Handle\n",
        "1. **Module replacement**: Replace `nn.RNN` with `nn.GRU` in your model definition\n",
        "2. **Output format**: GRU returns `(output, hidden)` just like vanilla RNN (no cell state!)\n",
        "   - The interface is identical: `packed_output, hidden = self.gru(packed_input)`\n",
        "3. **Final state extraction**: Use `hidden[0]` to get the final hidden state (same as RNN)\n",
        "\n",
        "### Hints\n",
        "- Start by copying the `CharRNNLightning` class and renaming it to `CharGRULightning`\n",
        "- Change `self.rnn` to `self.gru` and use `nn.GRU` instead of `nn.RNN`\n",
        "- The forward method is almost identical to the RNN version (no tuple unpacking needed like LSTM!)\n",
        "- GRU is a drop-in replacement for RNN with better performance\n",
        "\n",
        "### Expected Outcome\n",
        "After training, you should achieve similar or better test accuracy compared to both vanilla RNN and LSTM (~79-82%). GRUs often match LSTM performance while being more efficient to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbaec5b1",
      "metadata": {
        "id": "fbaec5b1"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}