{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f694453",
      "metadata": {
        "id": "3f694453"
      },
      "source": [
        "*<small>Last updated: 2026-02-25 01:35:27 UTC | Student Version (No Solutions)</small>*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cq0fhc1r26",
      "metadata": {
        "id": "6cq0fhc1r26"
      },
      "source": [
        "**Student Version** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Instructor Version**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/mtgca/7d52f0b7b63c6317f0151fe1505d85c7/CRNN.ipynb) &nbsp;&nbsp;&nbsp; [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtgca/DL-Labs/blob/main/02%20Sequential%20Models/CRNN.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d610a44-8b89-4ca9-ade1-54c987a64d49",
      "metadata": {
        "id": "3d610a44-8b89-4ca9-ade1-54c987a64d49"
      },
      "source": [
        "# CRNN usando PyTorch Lightning\n",
        "## Objetivos\n",
        "\n",
        "- Importar una base de datos de audio\n",
        "- Definir un modulo Dataset\n",
        "- Definir un modelo convolucional - recurrente para clasificacion de audio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a71b228-4837-4f2d-88b6-d61819f6c44f",
      "metadata": {
        "id": "1a71b228-4837-4f2d-88b6-d61819f6c44f"
      },
      "source": [
        "## Instalar e importar bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W3RBtN0Mhy4N",
      "metadata": {
        "id": "W3RBtN0Mhy4N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rcna7bxq6x7",
      "metadata": {
        "id": "rcna7bxq6x7"
      },
      "source": [
        "**PyDub** es una biblioteca de Python para manipulación de audio que facilita el trabajo con archivos de audio. Permite cargar, reproducir, convertir y editar archivos de audio en diferentes formatos (MP3, WAV, etc.) de manera simple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C_bmSIkHlzHj",
      "metadata": {
        "id": "C_bmSIkHlzHj"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yhwg0qi18c",
      "metadata": {
        "id": "yhwg0qi18c"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc22f9af-d33b-4a87-b57e-e4ee91431d50",
      "metadata": {
        "id": "cc22f9af-d33b-4a87-b57e-e4ee91431d50"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchmetrics\n",
        "from torchinfo import summary\n",
        "\n",
        "import torchaudio\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import TQDMProgressBar, RichProgressBar\n",
        "\n",
        "# Configurar semillas para reproducibilidad del split de datos\n",
        "torch.manual_seed(47)\n",
        "torch.cuda.manual_seed_all(47)\n",
        "np.random.seed(47)\n",
        "torch.version.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e5ab10",
      "metadata": {
        "id": "33e5ab10"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def is_remote_vscode():\n",
        "    \"\"\"Detect if running in VSCode remote container/SSH environment.\n",
        "\n",
        "    Uses DMI Product information to distinguish between VSCode Remote and Google Colab.\n",
        "    Google Colab runs on \"Google Compute Engine\" VMs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if os.path.exists('/sys/class/dmi/id/product_name'):\n",
        "            with open('/sys/class/dmi/id/product_name', 'r') as f:\n",
        "                product = f.read().strip()\n",
        "                print(\"DMI:\", product)\n",
        "                # Google Colab runs on Google Compute Engine\n",
        "                if 'Google' in product or 'Compute Engine' in product:\n",
        "                    return False  # This is Google Colab\n",
        "        # If not Google Compute Engine, assume VSCode Remote/Local\n",
        "        return True\n",
        "    except:\n",
        "        # If we can't read DMI, assume local/non-Colab\n",
        "        return True\n",
        "\n",
        "# Detect environment and set optimal configurations\n",
        "IS_REMOTE = is_remote_vscode()\n",
        "print(f\"Environment detected: {'VSCode Remote' if IS_REMOTE else 'Google Colab'}\")\n",
        "\n",
        "# Performance-optimized settings based on environment\n",
        "if IS_REMOTE:\n",
        "    NUM_WORKERS = 0  # Single-threaded to avoid IPC overhead\n",
        "    PIN_MEMORY = False  # Reduce memory pressure in container\n",
        "    PERSISTENT_WORKERS = False\n",
        "    PREFETCH_FACTOR = None\n",
        "    ENABLE_PROGRESS_BAR = False  # Disable progress bar updates over network\n",
        "    LOG_EVERY_N_STEPS = 50  # Reduce logging frequency\n",
        "else:\n",
        "    NUM_WORKERS = 2  # Leverage multiple cores for data loading\n",
        "    PIN_MEMORY = True  # Faster GPU transfer\n",
        "    PERSISTENT_WORKERS = True  # Reuse worker processes\n",
        "    PREFETCH_FACTOR = 2  # Load batches ahead\n",
        "    ENABLE_PROGRESS_BAR = True\n",
        "    LOG_EVERY_N_STEPS = 10\n",
        "print(f\"DataLoader num_workers set to: {NUM_WORKERS}\")\n",
        "print(\"Progress bar set to\", ENABLE_PROGRESS_BAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e08ccd4",
      "metadata": {
        "id": "9e08ccd4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "class SimpleEpochProgressCallback(pl.Callback):\n",
        "    \"\"\"Lightweight callback to show epoch progress without progress bar overhead.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.epoch_start_time = None\n",
        "\n",
        "    def on_train_start(self, trainer, pl_module):\n",
        "        \"\"\"Print training configuration at the start.\"\"\"\n",
        "        train_dataloader = trainer.train_dataloader\n",
        "\n",
        "        # Get batch size and number of batches\n",
        "        if hasattr(train_dataloader, 'batch_size'):\n",
        "            batch_size = train_dataloader.batch_size\n",
        "        elif hasattr(train_dataloader, 'loaders') and hasattr(train_dataloader.loaders, 'batch_size'):\n",
        "            batch_size = train_dataloader.loaders.batch_size\n",
        "        else:\n",
        "            batch_size = getattr(train_dataloader, 'batch_size', 'N/A')\n",
        "\n",
        "        num_batches = len(train_dataloader)\n",
        "\n",
        "        print(f\"\\nTraining Configuration: Batch Size={batch_size}, Steps per Epoch={num_batches}\\n\")\n",
        "\n",
        "    def on_train_epoch_start(self, trainer, pl_module):\n",
        "        \"\"\"Record the start time of the epoch.\"\"\"\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        \"\"\"Print single-line summary at the end of each training epoch.\"\"\"\n",
        "        current_epoch = trainer.current_epoch + 1\n",
        "        max_epochs = trainer.max_epochs\n",
        "\n",
        "        # Calculate epoch duration\n",
        "        epoch_time = time.time() - self.epoch_start_time if self.epoch_start_time else 0\n",
        "\n",
        "        # Get metrics from logged values\n",
        "        train_loss = trainer.callback_metrics.get('train_loss', None)\n",
        "        val_loss = trainer.callback_metrics.get('val_loss', None)\n",
        "        val_acc = trainer.callback_metrics.get('val_acc', None)\n",
        "\n",
        "        # Build single-line output\n",
        "        parts = [f\"Epoch {current_epoch}/{max_epochs}\"]\n",
        "\n",
        "        if train_loss is not None:\n",
        "            parts.append(f\"Train Loss: {train_loss:.4f}\")\n",
        "        if val_loss is not None:\n",
        "            parts.append(f\"Val Loss: {val_loss:.4f}\")\n",
        "        if val_acc is not None:\n",
        "            parts.append(f\"Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        parts.append(f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        print(\" | \".join(parts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2e6c83-95aa-4eca-8118-796435f54a17",
      "metadata": {
        "id": "6a2e6c83-95aa-4eca-8118-796435f54a17"
      },
      "source": [
        "## Definición de hiperparámetros de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465ba147-5255-4843-926d-35d4ab43fb06",
      "metadata": {
        "id": "465ba147-5255-4843-926d-35d4ab43fb06"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 200\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_WORKERS = 4\n",
        "CLASES = 10\n",
        "\n",
        "ANNOTATIONS_FILE = './UrbanSound8K/metadata/UrbanSound8K.csv'\n",
        "AUDIO_DIR = './UrbanSound8K/audio/'\n",
        "SAMPLE_RATE = 8000 # frecuencia de muestreo deseada\n",
        "MAX_LEN_SEC = 4  # longitud maxima del audio en segundos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cfbb7d4-7ade-4067-95eb-9d30fed83939",
      "metadata": {
        "id": "4cfbb7d4-7ade-4067-95eb-9d30fed83939"
      },
      "source": [
        "## Preparación de la base de datos: UrbanSound8k\n",
        "\n",
        "### Descargar UrbanSound8k y descomprimir en directorio './data'\n",
        "\n",
        "https://www.kaggle.com/datasets/chrisfilo/urbansound8k?resource=download\n",
        "\n",
        "### Informacion general del dataset en:\n",
        "\n",
        "https://urbansounddataset.weebly.com/urbansound8k.html\n",
        "\n",
        "### Visualizacion de una muestra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cYEUZAGvfbj5",
      "metadata": {
        "id": "cYEUZAGvfbj5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Verificar si el dataset ya existe\n",
        "if os.path.exists('./UrbanSound8K'):\n",
        "    print(\"El dataset UrbanSound8K ya existe. Omitiendo descarga.\")\n",
        "else:\n",
        "    print(\"Descargando dataset UrbanSound8K...\")\n",
        "    !wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz\n",
        "    print(\"Descomprimiendo dataset...\")\n",
        "    !tar -xzf urban8k.tgz\n",
        "    !rm urban8k.tgz\n",
        "    print(\"Descarga completada.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6181b5d2-bdc8-40f7-845b-cc354c43ac65",
      "metadata": {
        "id": "6181b5d2-bdc8-40f7-845b-cc354c43ac65"
      },
      "outputs": [],
      "source": [
        "all_csv = pd.read_csv(ANNOTATIONS_FILE)\n",
        "all_csv.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fztdv6ogioo",
      "metadata": {
        "id": "fztdv6ogioo"
      },
      "outputs": [],
      "source": [
        "# Distribución de las clases\n",
        "class_counts = all_csv['class'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "class_counts.plot(kind='bar', color='steelblue', edgecolor='black')\n",
        "plt.title('Distribución de Clases en UrbanSound8K', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Clase', fontsize=12)\n",
        "plt.ylabel('Número de muestras', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConteo por clase:\")\n",
        "print(class_counts)\n",
        "print(f\"\\nTotal de muestras: {len(all_csv)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rhl5je7y4es",
      "metadata": {
        "id": "rhl5je7y4es"
      },
      "outputs": [],
      "source": [
        "# Calcular la duración de cada audio en segundos\n",
        "all_csv['duration'] = all_csv['end'] - all_csv['start']\n",
        "\n",
        "# Crear histograma de la distribución de duraciones\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.hist(all_csv['duration'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "plt.axvline(MAX_LEN_SEC, color='red', linestyle='--', linewidth=2, label=f'MAX_LEN_SEC={MAX_LEN_SEC}s (umbral de preprocesamiento)')\n",
        "plt.yscale('log')  # Escala logarítmica en el eje Y\n",
        "plt.title('Distribución de Duración de Audios en UrbanSound8K (escala logarítmica)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Duración (segundos)', fontsize=12)\n",
        "plt.ylabel('Número de muestras (escala log)', fontsize=12)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Imprimir estadísticas descriptivas\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ESTADÍSTICAS DE DURACIÓN DE AUDIOS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Media:              {all_csv['duration'].mean():.3f}s\")\n",
        "print(f\"Mediana:            {all_csv['duration'].median():.3f}s\")\n",
        "print(f\"Desviación estándar: {all_csv['duration'].std():.3f}s\")\n",
        "print(f\"Mínimo:             {all_csv['duration'].min():.3f}s\")\n",
        "print(f\"Máximo:             {all_csv['duration'].max():.3f}s\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Analizar muestras respecto al umbral MAX_LEN_SEC\n",
        "over_threshold = (all_csv['duration'] > MAX_LEN_SEC).sum()\n",
        "under_threshold = (all_csv['duration'] <= MAX_LEN_SEC).sum()\n",
        "print(f\"\\nMuestras ≤ {MAX_LEN_SEC}s: {under_threshold} ({under_threshold/len(all_csv)*100:.1f}%)\")\n",
        "print(f\"Muestras > {MAX_LEN_SEC}s: {over_threshold} ({over_threshold/len(all_csv)*100:.1f}%) - serán truncadas\")\n",
        "print(f\"\\nTotal de muestras: {len(all_csv)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93914796-dd34-4253-bbd6-3cfa0840a040",
      "metadata": {
        "id": "93914796-dd34-4253-bbd6-3cfa0840a040"
      },
      "outputs": [],
      "source": [
        "fold = f\"fold{all_csv.iloc[0, 5]}\"\n",
        "path = os.path.join(AUDIO_DIR, fold, all_csv.iloc[0, 0])\n",
        "audio = AudioSegment.from_file(path)\n",
        "play(audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779dc9d4-86c0-4189-9eae-c290bcfab440",
      "metadata": {
        "id": "779dc9d4-86c0-4189-9eae-c290bcfab440"
      },
      "outputs": [],
      "source": [
        "# Load the audio file\n",
        "waveform, sample_rate = torchaudio.load(path)\n",
        "print(waveform.shape, sample_rate)\n",
        "# Create a time axis\n",
        "time_axis = torch.arange(0, waveform.shape[-1]) / sample_rate\n",
        "# Plot the waveform\n",
        "plt.plot(time_axis, waveform[0])\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Waveform')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b8cd54-1a2c-4f2a-9b65-aa927740ef68",
      "metadata": {
        "id": "32b8cd54-1a2c-4f2a-9b65-aa927740ef68"
      },
      "outputs": [],
      "source": [
        "# Definimos el computo del espectrograma de mel como una transformacion\n",
        "#https://docs.pytorch.org/audio/main/transforms.html\n",
        "\n",
        "mel_spectrogram = T.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=1024, # numero de puntos fft para transformar de tiempo a frecuencia\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "# aplicamos el espectrograma de mel\n",
        "mel_spectrogram_ = mel_spectrogram(waveform)\n",
        "\n",
        "# Convertimos a escala logaritmica (dB) para facilitar la visualizacion de detalles en todo el rango del espectrograma\n",
        "# Practica estandar en analisis de audio para destacar los picos y valles\n",
        "mel_spectrogram_db = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram_)\n",
        "\n",
        "# Plot mel spectrogram\n",
        "fig, axs = plt.subplots(1, 1)\n",
        "axs.set_title(\"Mel Spectrogram\")\n",
        "axs.set_ylabel(\"Frequency (Hz)\")\n",
        "axs.set_xlabel('Frame')\n",
        "im = axs.imshow(mel_spectrogram_db[0], origin='lower', aspect='auto')\n",
        "fig.colorbar(im, ax=axs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dztio3rqfl5",
      "metadata": {
        "id": "dztio3rqfl5"
      },
      "outputs": [],
      "source": [
        "def calculate_mel_spec_dims(max_len_sec, sample_rate, hop_length, n_mels):\n",
        "    \"\"\"\n",
        "    Calculate expected mel spectrogram dimensions based on audio parameters.\n",
        "\n",
        "    Formula explanation:\n",
        "    - Audio samples: max_len_sec * sample_rate\n",
        "    - Time frames: audio_samples // hop_length + 1\n",
        "      (The +1 accounts for center=True padding in MelSpectrogram)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (channels, n_mels, time_frames) - e.g., (1, 64, 63)\n",
        "    \"\"\"\n",
        "    num_samples = max_len_sec * sample_rate\n",
        "    time_frames = num_samples // hop_length + 1\n",
        "    return (1, n_mels, time_frames)\n",
        "\n",
        "# Calculate expected input dimensions automatically\n",
        "EXPECTED_INPUT_DIM = calculate_mel_spec_dims(\n",
        "    MAX_LEN_SEC,\n",
        "    SAMPLE_RATE,\n",
        "    mel_spectrogram.hop_length,  # 512\n",
        "    mel_spectrogram.n_mels        # 64\n",
        ")\n",
        "print(f\"Expected mel spectrogram shape: {EXPECTED_INPUT_DIM}\")\n",
        "print(f\"  - Channels: {EXPECTED_INPUT_DIM[0]}\")\n",
        "print(f\"  - Mel bins: {EXPECTED_INPUT_DIM[1]}\")\n",
        "print(f\"  - Time frames: {EXPECTED_INPUT_DIM[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2775zdwuf1i",
      "metadata": {
        "id": "2775zdwuf1i"
      },
      "source": [
        "### Relación entre MAX_LEN_SEC y dimensiones del mel spectrogram\n",
        "\n",
        "La longitud de la secuencia temporal (frames) del mel spectrogram se calcula automáticamente:\n",
        "\n",
        "**Fórmula:** `time_frames = (MAX_LEN_SEC × SAMPLE_RATE) ÷ hop_length + 1`\n",
        "\n",
        "**Ejemplo con parámetros actuales:**\n",
        "- MAX_LEN_SEC = 4 segundos\n",
        "- SAMPLE_RATE = 8000 Hz\n",
        "- hop_length = 512\n",
        "- time_frames = (4 × 8000) ÷ 512 + 1 = 63 frames\n",
        "\n",
        "**Nota:** El +1 es por el padding automático (`center=True`) en MelSpectrogram.\n",
        "\n",
        "Si cambias MAX_LEN_SEC, las dimensiones se actualizan automáticamente al re-ejecutar las celdas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i4_lXMWJum1N",
      "metadata": {
        "id": "i4_lXMWJum1N"
      },
      "outputs": [],
      "source": [
        "waveform.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b5fe02-2221-48fe-9eca-2300a1d5d202",
      "metadata": {
        "id": "27b5fe02-2221-48fe-9eca-2300a1d5d202"
      },
      "outputs": [],
      "source": [
        "mel_spectrogram_db.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a7804e1-7ba7-4be7-9d5a-3a9830bb9b05",
      "metadata": {
        "id": "7a7804e1-7ba7-4be7-9d5a-3a9830bb9b05"
      },
      "source": [
        "### Definicion de clase Dataset para UrbanSound8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679341d9-fb54-4154-a4dc-9b7ba3c78deb",
      "metadata": {
        "id": "679341d9-fb54-4154-a4dc-9b7ba3c78deb"
      },
      "outputs": [],
      "source": [
        "class UrbanSoundDataset(Dataset):\n",
        "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, max_len, training=True):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transformation = transformation\n",
        "        self.amplitude_to_db = T.AmplitudeToDB()  # Conversión a escala logarítmica (dB)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.max_len = max_len\n",
        "        self.training = training  # Para activar/desactivar data augmentation\n",
        "\n",
        "        # SpecAugment: solo para entrenamiento\n",
        "        if self.training:\n",
        "            self.freq_mask = T.FrequencyMasking(freq_mask_param=8)  # Enmascara hasta 8 mel bins\n",
        "            self.time_mask = T.TimeMasking(time_mask_param=15)      # Enmascara hasta 15 frames\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
        "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
        "        return path\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = T.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal): #reducimos dos canales de audio  (stereo) a monocanal\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    # Limitamos o rellenamos el audio a 4sec porque cada batch debe tener muestras con igual longitud\n",
        "    def _cut_if_necessary(self, signal):\n",
        "        if signal.shape[1] > self.max_len*self.target_sample_rate:\n",
        "            signal = signal[:, :self.max_len*self.target_sample_rate]\n",
        "        return signal\n",
        "\n",
        "    def _right_pad_if_necessary(self, signal):\n",
        "        length_signal = signal.shape[1]\n",
        "        if length_signal < self.max_len*self.target_sample_rate:\n",
        "            num_missing_samples = self.max_len*self.target_sample_rate - length_signal\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = F.pad(signal, last_dim_padding)\n",
        "        return signal\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        label = self.annotations.iloc[index, 6]\n",
        "\n",
        "        # Preprocesamiento de la señal\n",
        "        signal = self._resample_if_necessary(signal, sr)\n",
        "        signal = self._mix_down_if_necessary(signal)\n",
        "        signal = self._cut_if_necessary(signal)\n",
        "        signal = self._right_pad_if_necessary(signal)\n",
        "\n",
        "        # Calculamos el mel spectrogram\n",
        "        signal = self.transformation(signal)\n",
        "\n",
        "        # Convertir a escala dB (logarítmica)\n",
        "        signal = self.amplitude_to_db(signal)\n",
        "\n",
        "        # SpecAugment: aplicar frequency y time masking (solo durante entrenamiento)\n",
        "        if self.training:\n",
        "            # Frequency masking con 50% de probabilidad\n",
        "            if torch.rand(1).item() > 0.5:\n",
        "                signal = self.freq_mask(signal)\n",
        "\n",
        "            # Time masking con 50% de probabilidad\n",
        "            if torch.rand(1).item() > 0.5:\n",
        "                signal = self.time_mask(signal)\n",
        "\n",
        "        # Normalización (media=0, std=1) - siempre al final\n",
        "        mean = signal.mean()\n",
        "        std = signal.std()\n",
        "        signal = (signal - mean) / (std + 1e-8)  # epsilon para estabilidad numérica\n",
        "\n",
        "        return signal, label   #mel spec normalizado y aumentado, clase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0487e2-a81a-45d9-8879-cb76b0bb26dc",
      "metadata": {
        "id": "1a0487e2-a81a-45d9-8879-cb76b0bb26dc"
      },
      "outputs": [],
      "source": [
        "# Inicializamos la clase para el dataset\n",
        "us8k_dataset = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram, SAMPLE_RATE, MAX_LEN_SEC)\n",
        "\n",
        "# Inicializamos dataloader\n",
        "us8k_dataloader = DataLoader(us8k_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6433df91-e1a7-4e4d-b532-751524aca88e",
      "metadata": {
        "id": "6433df91-e1a7-4e4d-b532-751524aca88e"
      },
      "outputs": [],
      "source": [
        "print(\"Número de muestras en la base:\", len(us8k_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cebc09-52e0-4911-98cf-e1f37f69089d",
      "metadata": {
        "id": "08cebc09-52e0-4911-98cf-e1f37f69089d"
      },
      "outputs": [],
      "source": [
        "for signal, label in us8k_dataloader: #signal is a batch\n",
        "    break\n",
        "\n",
        "# Los datos ya están en dB y normalizados desde el dataloader\n",
        "# Plot mel spectrogram\n",
        "fig, axs = plt.subplots(1, 1, figsize=(10, 4))\n",
        "axs.set_title(\"Mel Spectrogram (Normalizado)\")\n",
        "axs.set_ylabel(\"Mel Bins\")\n",
        "axs.set_xlabel('Frame')\n",
        "im = axs.imshow(signal[0, 0], origin='lower', aspect='auto', cmap='viridis')\n",
        "fig.colorbar(im, ax=axs, label='Amplitud normalizada')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9a3c7d-8532-4da0-80e6-f1c1f8a1720f",
      "metadata": {
        "id": "4a9a3c7d-8532-4da0-80e6-f1c1f8a1720f"
      },
      "outputs": [],
      "source": [
        "# Dimensión de cada muestra\n",
        "print(\"Dimensión de entrada:\", signal.shape) # (batch_size, channels_in, mel_bins, temp_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lalorhkf0k",
      "metadata": {
        "id": "lalorhkf0k"
      },
      "outputs": [],
      "source": [
        "# Validar que las dimensiones de los datos coincidan con las esperadas\n",
        "actual_dims = signal.shape[1:]  # (channels, mel_bins, time_frames)\n",
        "\n",
        "if actual_dims == EXPECTED_INPUT_DIM:\n",
        "    print(f\"✓ Validación exitosa: Las dimensiones coinciden con las esperadas\")\n",
        "    print(f\"  Dimensiones: {actual_dims}\")\n",
        "else:\n",
        "    print(f\"✗ ADVERTENCIA: Discrepancia en dimensiones detectada!\")\n",
        "    print(f\"  Esperadas: {EXPECTED_INPUT_DIM}\")\n",
        "    print(f\"  Reales:    {actual_dims}\")\n",
        "    print(f\"  → Revisa los parámetros MAX_LEN_SEC, SAMPLE_RATE, n_fft, hop_length\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe3bf82-b16e-4466-930a-dba3d9f860e0",
      "metadata": {
        "id": "ffe3bf82-b16e-4466-930a-dba3d9f860e0"
      },
      "source": [
        "### Definición de UrbanSound8k DataModule para Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d884a62c-546b-46de-b889-988d4b94fe4e",
      "metadata": {
        "id": "d884a62c-546b-46de-b889-988d4b94fe4e"
      },
      "outputs": [],
      "source": [
        "class UrbanSound8KDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, csv_file, root_dir, mel_transf, target_sr, max_len_s, batch_size, num_workers):\n",
        "        super().__init__()\n",
        "        self.csv_file = csv_file\n",
        "        self.root_dir = root_dir\n",
        "        self.mel_transf = mel_transf\n",
        "        self.target_sr = target_sr\n",
        "        self.max_len_s = max_len_s\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Crear dataset temporal para obtener los índices del split\n",
        "        temp_dataset = UrbanSoundDataset(\n",
        "            self.csv_file, self.root_dir, self.mel_transf,\n",
        "            self.target_sr, self.max_len_s, training=False\n",
        "        )\n",
        "\n",
        "        # Calcular tamaños del split\n",
        "        train_size = int(0.8 * len(temp_dataset))\n",
        "        val_test_size = len(temp_dataset) - train_size\n",
        "        val_size = val_test_size // 2\n",
        "        test_size = val_test_size - val_size\n",
        "\n",
        "        # Split con generador fijo para reproducibilidad\n",
        "        generator = torch.Generator().manual_seed(47)\n",
        "        train_indices, val_indices, test_indices = random_split(\n",
        "            range(len(temp_dataset)),\n",
        "            [train_size, val_size, test_size],\n",
        "            generator=generator\n",
        "        )\n",
        "\n",
        "        # Crear datasets separados con augmentation SOLO para train\n",
        "        train_full = UrbanSoundDataset(\n",
        "            self.csv_file, self.root_dir, self.mel_transf,\n",
        "            self.target_sr, self.max_len_s, training=True  # Con SpecAugment\n",
        "        )\n",
        "        val_full = UrbanSoundDataset(\n",
        "            self.csv_file, self.root_dir, self.mel_transf,\n",
        "            self.target_sr, self.max_len_s, training=False  # Sin SpecAugment\n",
        "        )\n",
        "        test_full = UrbanSoundDataset(\n",
        "            self.csv_file, self.root_dir, self.mel_transf,\n",
        "            self.target_sr, self.max_len_s, training=False  # Sin SpecAugment\n",
        "        )\n",
        "\n",
        "        # Aplicar los índices del split usando Subset\n",
        "        from torch.utils.data import Subset\n",
        "        self.train_dataset = Subset(train_full, train_indices.indices)\n",
        "        self.val_dataset = Subset(val_full, val_indices.indices)\n",
        "        self.test_dataset = Subset(test_full, test_indices.indices)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5788f4f9-8127-4a39-a3e1-567c36c0a263",
      "metadata": {
        "id": "5788f4f9-8127-4a39-a3e1-567c36c0a263"
      },
      "outputs": [],
      "source": [
        "# Inicializamos el DataModule Lightning\n",
        "data_module = UrbanSound8KDataModule(csv_file = ANNOTATIONS_FILE, root_dir = AUDIO_DIR,\n",
        "                                     mel_transf = mel_spectrogram, target_sr = SAMPLE_RATE,\n",
        "                                     max_len_s = MAX_LEN_SEC, batch_size = BATCH_SIZE,\n",
        "                                     num_workers = NUM_WORKERS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62d61972-e9a5-440d-979c-ebf97c5986a1",
      "metadata": {
        "id": "62d61972-e9a5-440d-979c-ebf97c5986a1"
      },
      "source": [
        "## Definición de la arquitectura de la CRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1996d5ac-5750-498f-b65a-0972092c2210",
      "metadata": {
        "id": "1996d5ac-5750-498f-b65a-0972092c2210"
      },
      "outputs": [],
      "source": [
        "# Capa personalizada para hacer visible el reshape en torchinfo\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self, target_shape):\n",
        "        super(Reshape, self).__init__()\n",
        "        self.target_shape = target_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        return x.view(batch_size, *self.target_shape)\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes=10):\n",
        "        super(CRNN, self).__init__()\n",
        "\n",
        "        # Agrupar todas las capas convolucionales en un Sequential\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_dim[0], out_channels=16, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(16),  # Normalización después de Conv2d\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1)),  # Pool solo en frecuencia (height), preserva tiempo (width)\n",
        "            nn.Dropout2d(p=0.1),  # Regularización espacial\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(32),  # Normalización después de Conv2d\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1)),  # Pool solo en frecuencia (height), preserva tiempo (width)\n",
        "            nn.Dropout2d(p=0.25)  # Regularización espacial\n",
        "        )\n",
        "\n",
        "        # Inferir dimensiones automáticamente con un forward pass dummy\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, *input_dim)  # Shape: (1, channels_in, mel_bins, frames)\n",
        "            dummy_output = self.conv_layers(dummy_input)\n",
        "            # Después de conv_layers => Shape: (1, 32, height_reduced, width_preserved)\n",
        "            # Ejemplo con input (1, 1, 64, 63): dummy_output shape = (1, 32, 16, 63)\n",
        "\n",
        "            # Usar width como timesteps (frames temporales del mel spectrogram)\n",
        "            # width representa los frames temporales del audio (ahora se preservan todos)\n",
        "            self.seq_length = dummy_output.shape[3]  # width (frames temporales) => 63\n",
        "            self.rnn_input_dim = dummy_output.shape[1] * dummy_output.shape[2]  # channels * height => 32*16 = 512\n",
        "\n",
        "        # Crear Reshape con dimensiones inferidas\n",
        "        # Después de Reshape => Shape: (batch, seq_length, rnn_input_dim)\n",
        "        # Ejemplo: (batch, 63, 512) - cada timestep tiene 512 features\n",
        "        self.reshape = Reshape((self.seq_length, self.rnn_input_dim))\n",
        "\n",
        "        # LSTM bidireccional con 2 capas y dropout entre capas\n",
        "        # Input: (batch, seq_length, rnn_input_dim) => (batch, 63, 512)\n",
        "        # Output: (batch, seq_length, hidden_size*2) => (batch, 63, 256) porque bidirectional=True\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.rnn_input_dim,\n",
        "            hidden_size=128,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.25  # Dropout entre capas del LSTM (solo funciona con num_layers > 1)\n",
        "        )\n",
        "\n",
        "        # Dropout antes de la capa fully connected\n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "        # Segunda capa lineal: añadimos una capa oculta intermedia\n",
        "        # Input: (batch, 256) - último timestep del LSTM\n",
        "        # Output: (batch, 128) - capa oculta intermedia\n",
        "        self.fc1 = nn.Linear(128 * 2, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.25)\n",
        "\n",
        "        # Capa de salida para clasificación\n",
        "        # Input: (batch, 128) - capa oculta\n",
        "        # Output: (batch, num_classes) => (batch, 10)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (batch, 1, 64, 63) - mel spectrogram\n",
        "\n",
        "        # Aplicar capas convolucionales (con BatchNorm y Dropout)\n",
        "        x = self.conv_layers(x)\n",
        "        # Shape después: (batch, 32, 16, 63) - preservamos los 63 frames temporales\n",
        "        # 16 = 64 mel bins reducidos por pooling (2,1) dos veces: 64 -> 32 -> 16\n",
        "        # 63 = frames temporales preservados (no reducidos por pooling)\n",
        "\n",
        "        # Re-estructurar el tensor de salida usando la capa Reshape\n",
        "        x = self.reshape(x)\n",
        "        # Shape después: (batch, 63, 512)\n",
        "        # Ahora tenemos 63 timesteps, cada uno con 512 features (32 canales * 16 bins frecuencia)\n",
        "\n",
        "        # Aplicamos LSTM (con dropout interno)\n",
        "        x, _ = self.lstm(x)\n",
        "        # Shape después: (batch, 63, 256)\n",
        "        # 256 = 128*2 por el LSTM bidireccional\n",
        "\n",
        "        # Tomamos el ultimo time step de la secuencia del estado oculto\n",
        "        # El ultimo time step de la secuencia resume la representacion de toda la secuencia\n",
        "        x = x[:, -1, :]\n",
        "        # Shape después: (batch, 256)\n",
        "\n",
        "        # Average pooling a lo largo de la dimension temporal (alternativa comentada)\n",
        "        #x = torch.mean(x, dim=1)\n",
        "        # Shape después: (batch, 256)\n",
        "\n",
        "        # Aplicar dropout antes de la primera capa fully connected\n",
        "        x = self.dropout(x)\n",
        "        # Shape después: (batch, 256) - con dropout aplicado\n",
        "\n",
        "        # Primera capa fully connected con activación ReLU\n",
        "        x = self.fc1(x)\n",
        "        # Shape después: (batch, 128)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Aplicar segundo dropout\n",
        "        x = self.dropout2(x)\n",
        "        # Shape después: (batch, 128) - con dropout aplicado\n",
        "\n",
        "        # Segunda capa fully connected (capa de salida)\n",
        "        x = self.fc2(x)\n",
        "        # Shape después: (batch, 10) - logits para 10 clases\n",
        "\n",
        "        return x  # Devuelve logits (NO probabilidades)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc21cae7-9381-4150-aefd-f460075e54de",
      "metadata": {
        "id": "dc21cae7-9381-4150-aefd-f460075e54de"
      },
      "source": [
        "## Definición del Módulo Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd9a4e4b-b6a0-4156-a298-697911612ebb",
      "metadata": {
        "id": "cd9a4e4b-b6a0-4156-a298-697911612ebb"
      },
      "outputs": [],
      "source": [
        "class Lightning_CRNN(pl.LightningModule):\n",
        "    def __init__(self, input_dim, num_classes, lr):\n",
        "        super().__init__()\n",
        "        self.learning_rate = lr\n",
        "        self.classes = num_classes\n",
        "        self.model = CRNN(input_dim = input_dim, num_classes =  self.classes)\n",
        "\n",
        "        # Guardar hiperparametros en directorio de logs\n",
        "        # Ignora los pesos del modelo\n",
        "        self.save_hyperparameters(ignore=[\"model\"])\n",
        "\n",
        "        # Definición de métricas para cada grupo de datos\n",
        "        self.train_acc = torchmetrics.Accuracy(num_classes = self.classes, task='multiclass')\n",
        "        self.val_acc = torchmetrics.Accuracy(num_classes = self.classes, task='multiclass')\n",
        "        self.test_acc = torchmetrics.Accuracy(num_classes = self.classes, task='multiclass')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    # Pasos del proceso forward comunes entre train, val, test\n",
        "    def _shared_step(self, batch):\n",
        "        features, true_labels = batch\n",
        "        logits = self(features)\n",
        "        loss = torch.nn.functional.cross_entropy(logits, true_labels) # cross entropy loss recibe logits y labels como entrada. No recibe probabilidades!\n",
        "        predicted_labels = torch.argmax(logits, dim=1)\n",
        "\n",
        "        return loss, true_labels, predicted_labels\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.train_acc(predicted_labels, true_labels)\n",
        "        self.log(\"train_acc\", self.train_acc, on_epoch=True, on_step=False)\n",
        "        self.model.train()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        self.val_acc(predicted_labels, true_labels)\n",
        "        self.log(\"val_acc\", self.val_acc, on_epoch=True, on_step=False, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
        "        self.test_acc(predicted_labels, true_labels)\n",
        "        self.log(\"test_acc\", self.test_acc, on_epoch=True, on_step=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Optimizer con weight decay para L2 regularization\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=1e-4  # L2 regularization\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler: reduce LR cuando val_loss se estanca\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',           # Minimizar val_loss\n",
        "            factor=0.5,           # Reducir LR a la mitad\n",
        "            patience=5,           # Esperar 5 epochs sin mejora\n",
        "            #verbose=True,         # Imprimir cuando cambie el LR deprecated!\n",
        "            min_lr=1e-6          # LR mínimo\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\n",
        "                \"scheduler\": scheduler,\n",
        "                \"monitor\": \"val_loss\",  # Métrica a monitorear\n",
        "                \"interval\": \"epoch\",     # Evaluar cada epoch\n",
        "                \"frequency\": 1           # Evaluar cada 1 epoch\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb850fc-124f-452b-9bef-493c64a999da",
      "metadata": {
        "id": "beb850fc-124f-452b-9bef-493c64a999da"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac646688-80e2-4930-88da-b1f6130d7b03",
      "metadata": {
        "id": "ac646688-80e2-4930-88da-b1f6130d7b03"
      },
      "outputs": [],
      "source": [
        "# Inicialización del modulo lightning\n",
        "\n",
        "lightning_model = Lightning_CRNN(input_dim=EXPECTED_INPUT_DIM, num_classes=CLASES, lr=LEARNING_RATE)\n",
        "\n",
        "callback_check = ModelCheckpoint(save_top_k=1, save_last=True, mode=\"min\", monitor=\"val_loss\") # guardamos el mejor modelo monitoreado en la acc de validación. Por qué no la de entrenamiento?\n",
        "\n",
        "callback_early_stop = EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\", verbose=True)\n",
        "\n",
        "callback_tqdm = RichProgressBar(leave=True) #not used\n",
        "\n",
        "# Add epoch progress callback to callbacks list\n",
        "epoch_progress = SimpleEpochProgressCallback()\n",
        "\n",
        "logger = CSVLogger(save_dir=\"logs/\", name=\"crnn-2-urbansound8k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a213f5",
      "metadata": {
        "id": "50a213f5"
      },
      "outputs": [],
      "source": [
        "# Visualizar arquitectura del modelo\n",
        "print(\"Input size:\", (BATCH_SIZE, *signal.shape[1:]))\n",
        "print(\"Valores inferidos automáticamente:\")\n",
        "print(f\"  - seq_length: {lightning_model.model.seq_length}\")\n",
        "print(f\"  - rnn_input_dim: {lightning_model.model.rnn_input_dim}\")\n",
        "print()\n",
        "summary(lightning_model.model, input_size=(BATCH_SIZE, *signal.shape[1:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4ae7f2",
      "metadata": {
        "id": "0c4ae7f2"
      },
      "source": [
        "### Explicación del Cálculo de Parámetros del LSTM\n",
        "\n",
        "El LSTM en este modelo tiene **1,052,672 parámetros**. Veamos cómo se calculan:\n",
        "\n",
        "#### Configuración del LSTM\n",
        "- `input_size = 512` (rnn_input_dim: dimensión de entrada)\n",
        "- `hidden_size = 128` (tamaño del estado oculto)\n",
        "- `num_layers = 2` (dos capas apiladas)\n",
        "- `bidirectional = True` (procesa la secuencia en ambas direcciones)\n",
        "\n",
        "#### Fórmula General\n",
        "\n",
        "Para cada capa LSTM en una dirección, los parámetros incluyen:\n",
        "- **W_ih** (pesos entrada-a-oculto): `4 × hidden_size × input_size`\n",
        "- **W_hh** (pesos oculto-a-oculto): `4 × hidden_size × hidden_size`\n",
        "- **b_ih** (bias entrada-a-oculto): `4 × hidden_size`\n",
        "- **b_hh** (bias oculto-a-oculto): `4 × hidden_size`\n",
        "\n",
        "El factor **4** proviene de las cuatro puertas del LSTM: *input gate*, *forget gate*, *output gate* y *cell state*.\n",
        "\n",
        "**Fórmula simplificada para una dirección:**\n",
        "```\n",
        "params = 4 × hidden_size × (input_size + hidden_size + 2)\n",
        "```\n",
        "Donde el \"+2\" representa los dos vectores de bias (b_ih y b_hh).\n",
        "\n",
        "#### Desglose por Capa\n",
        "\n",
        "**Capa 1** (recibe entrada de las convoluciones):\n",
        "- `input_size = 512`, `hidden_size = 128`\n",
        "- **Dirección forward**: `4 × 128 × (512 + 128 + 2) = 512 × 642 = 328,704`\n",
        "- **Dirección backward** (bidirectional): `328,704`\n",
        "- **Total Capa 1: 657,408 parámetros**\n",
        "\n",
        "**Capa 2** (recibe salida bidireccional de Capa 1):\n",
        "- `input_size = 256` (128 × 2, ya que la capa anterior es bidireccional)\n",
        "- `hidden_size = 128`\n",
        "- **Dirección forward**: `4 × 128 × (256 + 128 + 2) = 512 × 386 = 197,632`\n",
        "- **Dirección backward**: `197,632`\n",
        "- **Total Capa 2: 395,264 parámetros**\n",
        "\n",
        "#### Total\n",
        "```\n",
        "LSTM total = Capa 1 + Capa 2 = 657,408 + 395,264 = 1,052,672 ✓\n",
        "```\n",
        "\n",
        "Este resultado coincide exactamente con los parámetros mostrados en el resumen del modelo arriba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c7970a",
      "metadata": {
        "id": "03c7970a"
      },
      "source": [
        "### Configuración de Checkpoint Recovery\n",
        "Si tu entrenamiento se interrumpe, puedes recuperarlo desde el último checkpoint guardado. PyTorch Lightning guarda automáticamente checkpoints en la carpeta `logs/` (o la que hayas configurado previamente).\n",
        "El entrenamiento soporta **3 modos** :\n",
        "\n",
        "#### Modo 1: Entrenar desde cero (por defecto)\n",
        "```python\n",
        "CHECKPOINT_PATH = None\n",
        "LOAD_WEIGHTS_ONLY = False\n",
        "```\n",
        "\n",
        "#### Modo 2: Continuar entrenamiento desde checkpoint (RECOMENDADO)\n",
        "Mantiene el estado completo: epoch actual, optimizer, scheduler, etc.\n",
        "```python\n",
        "CHECKPOINT_PATH = \"logs/crnn-2-urbansound8k/version_26/checkpoints/epoch=62-step=13797.ckpt\"\n",
        "LOAD_WEIGHTS_ONLY = False\n",
        "```\n",
        "\n",
        "#### Modo 3: Cargar solo pesos del modelo\n",
        "Útil para transfer learning o fine-tuning. Reinicia epoch, optimizer y scheduler.\n",
        "```python\n",
        "CHECKPOINT_PATH = \"logs/crnn-2-urbansound8k/version_26/checkpoints/epoch=62-step=13797.ckpt\"\n",
        "LOAD_WEIGHTS_ONLY = True\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99f1d75-7ed9-4dee-b732-4c403443fca5",
      "metadata": {
        "id": "b99f1d75-7ed9-4dee-b732-4c403443fca5"
      },
      "outputs": [],
      "source": [
        "# ========== TRAINING CONFIGURATION ==========\n",
        "# Configure checkpoint recovery here\n",
        "\n",
        "CHECKPOINT_PATH = None  # Set to checkpoint path (e.g., \"logs/.../epoch=62.ckpt\") or None for training from scratch\n",
        "#CHECKPOINT_PATH = \"logs/crnn-2-urbansound8k/version_26/checkpoints/epoch=62-step=13797.ckpt\"\n",
        "LOAD_WEIGHTS_ONLY = False  # True = load weights but restart training (epoch=0, reset optimizer/scheduler, callback state)\n",
        "\n",
        "# Examples:\n",
        "# CHECKPOINT_PATH = None                                  # Train from scratch\n",
        "# CHECKPOINT_PATH = \"logs/.../epoch=62.ckpt\"              # Resume training (LOAD_WEIGHTS_ONLY=False)\n",
        "# CHECKPOINT_PATH = \"logs/.../epoch=62.ckpt\" + LOAD_WEIGHTS_ONLY=True  # Load weights, restart training\n",
        "\n",
        "# ============================================\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=NUM_EPOCHS,\n",
        "                    callbacks=[callback_check, callback_early_stop, epoch_progress],\n",
        "                    accelerator=\"auto\",\n",
        "                    devices=\"auto\",\n",
        "                    logger=logger,\n",
        "                    deterministic=False,\n",
        "                    log_every_n_steps=10,\n",
        "                    enable_progress_bar=ENABLE_PROGRESS_BAR)\n",
        "\n",
        "if CHECKPOINT_PATH is None:\n",
        "    # Mode: Train from scratch\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Training mode: From scratch\")\n",
        "    print(\"=\" * 60)\n",
        "    start_time = time.time()\n",
        "    trainer.fit(model=lightning_model, datamodule=data_module)\n",
        "\n",
        "elif not LOAD_WEIGHTS_ONLY:\n",
        "    # Mode: Resume from checkpoint (continues epoch, optimizer, scheduler state, ...\n",
        "    # checkpoint states such as early stop are restored (e.g. patience) to their original values even if changed)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Training mode: Resume from checkpoint\")\n",
        "    print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "    print(\"=\" * 60)\n",
        "    start_time = time.time()\n",
        "    trainer.fit(model=lightning_model, datamodule=data_module, ckpt_path=CHECKPOINT_PATH)\n",
        "\n",
        "else:\n",
        "    # Mode: Load weights only (start from epoch 0 with pretrained weights)\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Training mode: Load pretrained weights\")\n",
        "    print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load model from checkpoint\n",
        "    lightning_model = Lightning_CRNN.load_from_checkpoint(\n",
        "        CHECKPOINT_PATH,\n",
        "        input_dim=EXPECTED_INPUT_DIM,\n",
        "        num_classes=CLASES,\n",
        "        lr=LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    # Train from epoch 0 with pretrained weights\n",
        "    start_time = time.time()\n",
        "    trainer.fit(model=lightning_model, datamodule=data_module)\n",
        "\n",
        "runtime = (time.time() - start_time) / 60\n",
        "print(f\"\\nTiempo de entrenamiento en minutos: {runtime:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9272ebf2-793c-45ba-8a6a-047f0e7a15a4",
      "metadata": {
        "id": "9272ebf2-793c-45ba-8a6a-047f0e7a15a4"
      },
      "source": [
        "## Graficamos las curvas de aprendizaje del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8dc6c5-1475-418a-ad11-585626f96729",
      "metadata": {
        "id": "cd8dc6c5-1475-418a-ad11-585626f96729"
      },
      "outputs": [],
      "source": [
        "# Determine which metrics.csv to read based on checkpoint usage\n",
        "if CHECKPOINT_PATH is not None and LOAD_WEIGHTS_ONLY is False:\n",
        "    # Extract the log directory from the checkpoint path\n",
        "    # Path format: \"logs/crnn-2-urbansound8k/version_X/checkpoints/epoch=Y.ckpt\"\n",
        "    checkpoint_log_dir = os.path.dirname(os.path.dirname(CHECKPOINT_PATH))\n",
        "    metrics_path = f\"{checkpoint_log_dir}/metrics.csv\"\n",
        "    print(f\"Loading metrics from checkpoint's original training run:\")\n",
        "    print(f\"  {metrics_path}\")\n",
        "else:\n",
        "    # Use current training run\n",
        "    metrics_path = f\"{trainer.logger.log_dir}/metrics.csv\"\n",
        "    print(f\"Loading metrics from current training run:\")\n",
        "    print(f\"  {metrics_path}\")\n",
        "\n",
        "metrics = pd.read_csv(metrics_path)\n",
        "\n",
        "aggreg_metrics = []\n",
        "agg_col = \"epoch\"\n",
        "for i, dfg in metrics.groupby(agg_col):\n",
        "    agg = dict(dfg.mean())\n",
        "    agg[agg_col] = i\n",
        "    aggreg_metrics.append(agg)\n",
        "\n",
        "df_metrics = pd.DataFrame(aggreg_metrics)\n",
        "df_metrics[[\"train_loss\", \"val_loss\"]].plot(\n",
        "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"Loss\"\n",
        ")\n",
        "df_metrics[[\"train_acc\", \"val_acc\"]].plot(\n",
        "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"ACC\"\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b88cdae-a3d5-4d7d-9f47-7c6643ed6d99",
      "metadata": {
        "id": "2b88cdae-a3d5-4d7d-9f47-7c6643ed6d99"
      },
      "source": [
        "## Evaluamos el mejor modelo en el grupo de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cf46a1-01a6-4964-bd59-ca3e4b3f006a",
      "metadata": {
        "id": "10cf46a1-01a6-4964-bd59-ca3e4b3f006a"
      },
      "outputs": [],
      "source": [
        "trainer.test(model = lightning_model, datamodule = data_module, ckpt_path = 'best') # cargamos el mejor checkpoint del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activity_header",
      "metadata": {
        "id": "activity_header"
      },
      "source": [
        "# 🎯 Actividad para Estudiantes: Clasificación de Sonidos Ambientales con ESC-50\n",
        "\n",
        "En esta actividad, aplicarás los conceptos aprendidos sobre CRNN para resolver un nuevo problema de clasificación de audio utilizando el dataset **ESC-50**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activity_objective",
      "metadata": {
        "id": "activity_objective"
      },
      "source": [
        "## Objetivo\n",
        "\n",
        "Implementar y entrenar un modelo CRNN para clasificar sonidos ambientales del dataset **ESC-50** (Environmental Sound Classification), aplicando técnicas de aumento de datos tanto en el dominio de la forma de onda como en el dominio tiempo-frecuencia.\n",
        "\n",
        "## Dataset: ESC-50\n",
        "\n",
        "**ESC-50** es un dataset etiquetado de sonidos ambientales que contiene 2000 grabaciones de audio organizadas en 50 clases semánticas (40 clips por clase).\n",
        "\n",
        "- **Fuente**: [ESC-50 en Kaggle](https://www.kaggle.com/datasets/mmoreaux/environmental-sound-classification-50)\n",
        "- **Estructura**: 2000 archivos de audio de 5 segundos cada uno\n",
        "- **Clases**: 50 categorías de sonidos ambientales (animales, sonidos naturales, interiores/exteriores, humanos, etc.)\n",
        "- **Formato**: Archivos `.wav` con frecuencia de muestreo original de 44.1 kHz\n",
        "\n",
        "**Nota**: Necesitarás descargar el dataset desde Kaggle y extraerlo en tu directorio de trabajo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activity_requirements",
      "metadata": {
        "id": "activity_requirements"
      },
      "source": [
        "## Requisitos Técnicos\n",
        "\n",
        "Tu implementación debe incluir los siguientes componentes:\n",
        "\n",
        "### 1. Preparación del Dataset\n",
        "- Descargar y explorar el dataset ESC-50\n",
        "- Analizar la estructura de archivos y metadatos (archivo CSV)\n",
        "- Adaptar la clase `UrbanSoundDataset` para trabajar con ESC-50\n",
        "\n",
        "### 2. Implementación de Aumento de Datos\n",
        "\n",
        "Debes implementar **al menos dos técnicas de aumento de datos (una en el dominio de la forma de onda y otra en el dominio de tiempo frecuencia)**:\n",
        "\n",
        "#### a) Aumento en el dominio de la forma de onda (waveform)\n",
        "Elige al menos una técnica que opere directamente sobre la señal de audio en el tiempo. Por ejemplo:\n",
        "- **Time Stretching**: Modificar la velocidad del audio sin cambiar el pitch\n",
        "- **Pitch Shifting**: Cambiar el tono sin modificar la velocidad\n",
        "- **Time Shift**: Desplazar la señal en el tiempo (rotar circularmente)\n",
        "- **Adición de ruido**: Agregar ruido gaussiano o ruido de fondo\n",
        "- **Cambio de volumen**: Amplificación o atenuación aleatoria\n",
        "\n",
        "#### b) Aumento en el dominio tiempo-frecuencia (spectrogram)\n",
        "Elige al menos una técnica que opere sobre el espectrograma de mel. Por ejemplo:\n",
        "- **SpecAugment**: Ya viste frequency masking y time masking en el notebook (puedes variar los parámetros)\n",
        "- **Cutout**: Enmascarar regiones rectangulares aleatorias del espectrograma\n",
        "- **MixUp**: Mezclar dos espectrogramas y sus etiquetas\n",
        "- **Normalización por banda**: Normalizar cada banda de frecuencia independientemente\n",
        "\n",
        "### 3. Arquitectura y Entrenamiento\n",
        "- Utilizar la arquitectura CRNN ya definida (puedes ajustar hiperparámetros)\n",
        "- Adaptar el modelo para las **50 clases** de ESC-50\n",
        "- Entrenar el modelo con tus técnicas de aumento de datos\n",
        "- Monitorear las métricas de entrenamiento y validación\n",
        "\n",
        "### 4. Evaluación y Comparación\n",
        "- Evaluar el modelo en el conjunto de test\n",
        "- Analizar las curvas de aprendizaje y discutir los resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activity_hints",
      "metadata": {
        "id": "activity_hints"
      },
      "source": [
        "## Pistas y Consideraciones\n",
        "\n",
        "### Diferencias clave con UrbanSound8K\n",
        "- ESC-50 tiene **50 clases** en lugar de 10\n",
        "- Los audios duran **5 segundos** (vs. duraciones variables en UrbanSound8K)\n",
        "- La estructura del archivo CSV es diferente (revisa las columnas disponibles)\n",
        "- Aunque El dataset ya está organizado en 5 folds para cross-validation, no es necesario que los uses. Puedes hacer una división básica (entrenamiento, validación y test) como la realizada en esta notebook\n",
        "\n",
        "### Bibliotecas útiles para aumento de datos\n",
        "\n",
        "**Aumento en waveform:**\n",
        "```python\n",
        "# Con torchaudio.transforms:\n",
        "T.TimeStretch()       # Estirar/comprimir tiempo\n",
        "T.PitchShift()        # Cambiar pitch\n",
        "# Agregar ruido: torch.randn() * factor\n",
        "# Time shift: torch.roll(waveform, shifts=n, dims=-1)\n",
        "```\n",
        "\n",
        "**Otras opciones:**\n",
        "- `audiomentations`: Biblioteca especializada en augmentations de audio\n",
        "- `torch-audiomentations`: Versión PyTorch de audiomentations\n",
        "\n",
        "### Consideraciones importantes\n",
        "\n",
        "1. **Longitud del audio**: ESC-50 tiene audios de 5 segundos. Ajusta `MAX_LEN_SEC` según corresponda.\n",
        "\n",
        "2. **Frecuencia de muestreo**: Los archivos originales están a 44.1 kHz. Decide si quieres resamplear a una frecuencia menor (como hicimos con 8 kHz en UrbanSound8K) para reducir el cómputo.\n",
        "\n",
        "3. **Data augmentation selectivo**: Aplica augmentation **solo durante entrenamiento**, no durante validación/test.\n",
        "\n",
        "4. **Probabilidad de augmentation**: No apliques todas las técnicas a la vez. Usa probabilidades (ej. 50% de aplicar cada técnica) para mantener variabilidad.\n",
        "\n",
        "5. **Normalización**: Aplica la normalización (media=0, std=1) **después** de todas las transformaciones.\n",
        "\n",
        "6. **Dimensiones de entrada**: Con 5 segundos de audio, las dimensiones del espectrograma cambiarán. Asegúrate de calcular `EXPECTED_INPUT_DIM` correctamente.\n",
        "\n",
        "7. **Hiperparámetros**: Con más clases (50 vs 10), podrías necesitar:\n",
        "   - Más epochs para convergencia\n",
        "   - Learning rate diferente\n",
        "   - Batch size ajustado según memoria disponible\n",
        "   - Una arquitectura diferente\n",
        "\n",
        "8. **Benchmarks**: Mira el performance alcanzado por otras arquitecturas como referencia para tener una idea del performance mínimo esperado\n",
        "https://github.com/karolpiczak/ESC-50\n",
        "\n",
        "### Debugging\n",
        "- Visualiza las formas de onda y espectrogramas **antes y después** del augmentation para verificar que funcionen correctamente\n",
        "- Verifica las dimensiones de los tensores en cada paso\n",
        "- Compara la distribución de clases en train/val/test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "activity_deliverables",
      "metadata": {
        "id": "activity_deliverables"
      },
      "source": [
        "## Entregables\n",
        "\n",
        "Tu solución debe incluir:\n",
        "\n",
        "1. **Código implementado**:\n",
        "   - Clase `Dataset` adaptada para ESC-50\n",
        "   - Implementación de al menos 2 técnicas de aumento de datos (una en waveform, una en spectrogram)\n",
        "   - DataModule de PyTorch Lightning para ESC-50\n",
        "   - Entrenamiento del modelo CRNN\n",
        "\n",
        "2. **Visualizaciones**:\n",
        "   - Distribución de clases en ESC-50\n",
        "   - Ejemplos de formas de onda antes/después del augmentation\n",
        "   - Ejemplos de espectrogramas antes/después del augmentation\n",
        "   - Curvas de aprendizaje (loss y accuracy para train/val)\n",
        "\n",
        "3. **Resultados**:\n",
        "   - Accuracy final en el conjunto de test\n",
        "\n",
        "4. **Análisis** (breve, 2-3 párrafos máximo):\n",
        "   - ¿Qué desafíos encontraste al trabajar con 50 clases vs 10?\n",
        "   - ¿Qué mejoras propondrías para aumentar el accuracy?\n",
        "\n",
        "## Criterios de Evaluación\n",
        "\n",
        "- **Correctitud técnica** (40%): El código funciona correctamente y sigue las buenas prácticas\n",
        "- **Implementación de augmentation** (30%): Al menos 2 técnicas implementadas correctamente\n",
        "- **Análisis de resultados** (20%): Discusión clara y fundamentada de los resultados\n",
        "- **Visualizaciones** (10%): Gráficas claras y bien documentadas\n",
        "\n",
        "---\n",
        "\n",
        "**¡Buena suerte!** Recuerda que el objetivo no es solo obtener alta precisión, sino entender cómo las diferentes técnicas de procesamiento y aumento de datos afectan el desempeño del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "activity_code_placeholder",
      "metadata": {
        "id": "activity_code_placeholder"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TU CÓDIGO AQUÍ / YOUR CODE HERE\n",
        "# ============================================\n",
        "\n",
        "# Comienza descargando el dataset ESC-50 y explorando su estructura\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}