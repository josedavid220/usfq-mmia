{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fec8fef7"
      },
      "source": [
        "*<small>Last updated: 2026-02-19 22:39:14 UTC | Student Version (No Solutions)</small>*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oTwRqDX9jgu"
      },
      "source": [
        "**Student Version** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **Instructor Version**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/mtgca/7d52f0b7b63c6317f0151fe1505d85c7/ComplexCNN.ipynb) &nbsp;&nbsp;&nbsp; [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mtgca/DL-Labs/blob/main/01%20CNNs/ComplexCNN.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPAcSBerZalE"
      },
      "source": [
        "# MobileNetV2 usando PyTorch Lightning\n",
        "## Objetivos\n",
        "\n",
        "- Importar un modelo CNN complejo\n",
        "- Entrenar el modelo para clasificacion de imagenes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulGyz-4eZowy"
      },
      "source": [
        "## Instalar e importar bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ070UkBhBao"
      },
      "outputs": [],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT1MMnNbZVKr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics\n",
        "\n",
        "from collections import Counter\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning.callbacks import TQDMProgressBar, RichProgressBar\n",
        "\n",
        "torch.set_float32_matmul_precision('medium')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G87pPy5y7uWc"
      },
      "source": [
        "# Performance Optimization Through Environment Detection\n",
        "The fundamental difference between VSCode remote containers and Google Colab lies in how they communicate with the user interface. In VSCode remote environments, your code executes inside a Docker container or remote server, and all output—including progress bar updates, print statements, and data transfers—must traverse the network to reach your local VSCode client. When PyTorch Lightning's progress bar updates hundreds of times per epoch (once per batch), these frequent UI updates create massive network overhead, with each update requiring a round-trip between the container and your client. Similarly, spawning multiple data loader worker processes in a remote container creates severe inter-process communication (IPC) bottlenecks, as each worker must serialize and transfer data through multiple layers. In contrast, Google Colab runs in a browser with a local connection to its backend servers, making these operations far less costly.\n",
        "\n",
        "This environment detection code automatically identifies whether you're in a constrained remote environment and adjusts critical settings accordingly. For VSCode remote, it sets num_workers=0 to eliminate multi-process overhead (letting the GPU handle parallelism instead), disables progress_bar to eliminate hundreds of network round-trips per epoch, and reduces logging frequency by 10x. These changes transform training from 10-100x slower than native to near-native GPU speed—typically resulting in 5-10x performance improvement. For Colab or local environments where these bottlenecks don't exist, it maintains standard settings with progress bars and parallel data loading. This automatic adaptation means you can run the same notebook efficiently in both environments without manual configuration changes, ensuring optimal performance regardless of where you execute your training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zC1pl-AN7q4d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "def is_remote_vscode():\n",
        "    \"\"\"Detect if running in VSCode remote container/SSH environment.\n",
        "\n",
        "    Uses DMI Product information to distinguish between VSCode Remote and Google Colab.\n",
        "    Google Colab runs on \"Google Compute Engine\" VMs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if os.path.exists('/sys/class/dmi/id/product_name'):\n",
        "            with open('/sys/class/dmi/id/product_name', 'r') as f:\n",
        "                product = f.read().strip()\n",
        "                print(\"DMI:\", product)\n",
        "                # Google Colab runs on Google Compute Engine\n",
        "                if 'Google' in product or 'Compute Engine' in product:\n",
        "                    return False  # This is Google Colab\n",
        "        # If not Google Compute Engine, assume VSCode Remote/Local\n",
        "        return True\n",
        "    except:\n",
        "        # If we can't read DMI, assume local/non-Colab\n",
        "        return True\n",
        "\n",
        "# Detect environment and set optimal configurations\n",
        "IS_REMOTE = is_remote_vscode()\n",
        "print(f\"Environment detected: {'VSCode Remote' if IS_REMOTE else 'Google Colab'}\")\n",
        "\n",
        "# Performance-optimized settings based on environment\n",
        "if IS_REMOTE:\n",
        "    ENABLE_PROGRESS_BAR = False  # Disable progress bar updates over network\n",
        "else:\n",
        "    ENABLE_PROGRESS_BAR = True\n",
        "print(f\"Progress bar enabled: {ENABLE_PROGRESS_BAR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOZzWCsUbzLj"
      },
      "source": [
        "## Definición de hiperparámetros de la red"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5M5fDxUb4f7"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 60\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_WORKERS = 4\n",
        "CLASES = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N5UHClkbWGU"
      },
      "source": [
        "## Preparación de la base de datos: CIFAR-10\n",
        "\n",
        "### CIFAR-10 contiene 60k RGB imágenes de 32x32x3 pixeles distribuidas en 10 clases.\n",
        "\n",
        "*   Grupo de entrenamiento: 50K imágenes\n",
        "*   Grupo de evaluación: 10k imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiOprzBWbdOD"
      },
      "outputs": [],
      "source": [
        "# Descargamos dataset\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, transform=transforms.ToTensor(), download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmJfI9zDbdIU"
      },
      "outputs": [],
      "source": [
        "# Definimos dataloaders para los datasets\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          drop_last=False, # ignora el último batch si el número de muestras no son divisibles para el tamaño de batch\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          num_workers=NUM_WORKERS,\n",
        "                          drop_last=False,\n",
        "                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTvckCDbdBj"
      },
      "outputs": [],
      "source": [
        "print(\"Número de muestras de entrenamiento:\", len(train_dataset))\n",
        "print(\"Número de muestras de evaluación:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUTK_l3EhHoX"
      },
      "outputs": [],
      "source": [
        "print(\"Número de iteraciones por época:\", len(train_loader))\n",
        "# 1563*32 = 50000\n",
        "# 3152*16 = 50000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHzrj7uPhCBv"
      },
      "outputs": [],
      "source": [
        "# Visualización de imágenes\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    break\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Imágenes de entrenamiento\")\n",
        "plt.imshow(np.transpose(torchvision.utils.make_grid(\n",
        "    images[:64],\n",
        "    padding=2,\n",
        "    normalize=True),\n",
        "    (1, 2, 0)))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeeVYtR8i6pe"
      },
      "outputs": [],
      "source": [
        "# Dimensión de cada imagen\n",
        "print(\"Dimensión de entrada:\", images.shape) # (batch_size, channels_in, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4Ua4znjYfW"
      },
      "source": [
        "### Definición de CIFAR-10 DataModule para Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTycs3Z6jY1H"
      },
      "outputs": [],
      "source": [
        "class CIFAR_DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_path=\"./\", batch_size=32):\n",
        "        super().__init__()\n",
        "        self.data_path = data_path\n",
        "        self.batch_size = batch_size\n",
        "        self.train_transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((256, 256)),\n",
        "            torchvision.transforms.RandomCrop((224, 224)),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(\n",
        "                (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "        self.test_transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((224, 224)),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Normalize(\n",
        "                (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "        #https://pytorch.org/hub/pytorch_vision_mobilenet_v2/ see normalization values\n",
        "\n",
        "    def prepare_data(self): # Método prepare_data es usado para pasos que van a ser ejecutados solo una vez, como descargar el dataset y definir las transformaciones\n",
        "\n",
        "        #datasets.CIFAR10(root=self.data_path, download=True)\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    def setup(self, stage=None):   # cargamos el dataset\n",
        "        train = datasets.CIFAR10(\n",
        "            root=self.data_path,\n",
        "            train=True,\n",
        "            transform=self.train_transform,\n",
        "            download=False,\n",
        "        )\n",
        "\n",
        "        self.test = datasets.CIFAR10(\n",
        "            root=self.data_path,\n",
        "            train=False, # False para obtener el grupo de test\n",
        "            transform=self.test_transform,\n",
        "            download=False,\n",
        "        )\n",
        "\n",
        "        # Dividir el grupo de entrenamiento original en entrenamiento y validación\n",
        "\n",
        "        self.train, self.valid = random_split(train, lengths=[int(len(train)*0.9), int(len(train)*0.1)])\n",
        "\n",
        "        print(\"Muestras de entrenamiento:\", len(self.train))\n",
        "        print(\"Muestras de validación:\", len(self.valid))\n",
        "        print(\"Muestras de evaluación:\", len(self.test))\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_loader = DataLoader(\n",
        "            dataset=self.train,\n",
        "            batch_size=self.batch_size,\n",
        "            drop_last=True,\n",
        "            shuffle=True,\n",
        "            num_workers=NUM_WORKERS,\n",
        "        )\n",
        "        return train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        valid_loader = DataLoader(\n",
        "            dataset=self.valid,\n",
        "            batch_size=self.batch_size,\n",
        "            drop_last=False,\n",
        "            shuffle=False,\n",
        "            num_workers=NUM_WORKERS,\n",
        "        )\n",
        "        return valid_loader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        test_loader = DataLoader(\n",
        "            dataset=self.test,\n",
        "            batch_size=self.batch_size,\n",
        "            drop_last=False,\n",
        "            shuffle=False,\n",
        "            num_workers=NUM_WORKERS,\n",
        "        )\n",
        "        return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubTECZ79mg8E"
      },
      "outputs": [],
      "source": [
        "# Inicizalización de DataModule\n",
        "\n",
        "torch.manual_seed(47)  # especificamos un random seed para reproducibilidad de inicializaciones aleatorias\n",
        "data_module = CIFAR_DataModule(data_path='./data', batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhuT7Jf6ZtjN"
      },
      "source": [
        "## Importar la arquitectura MobileNetV2\n",
        "\n",
        "https://pytorch.org/hub/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUxB_MGA5ZB3"
      },
      "outputs": [],
      "source": [
        "pytorch_model = torch.hub.load('pytorch/vision:v0.11.0', 'mobilenet_v2', weights=None) # modelo sin pre-entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erEHHmvW5ZB4"
      },
      "source": [
        "Modificamos el numero de clases de salida en MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3-S9jjl5ZB5"
      },
      "outputs": [],
      "source": [
        "pytorch_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_TPK_tKkFOP"
      },
      "outputs": [],
      "source": [
        "# Example input tensor with shape [32, 3, 224, 224]\n",
        "input_tensor = torch.randn(32, 3, 224, 224)\n",
        "# Forward pass through the feature extractor (excluding classifier)\n",
        "features_ex = pytorch_model.features(input_tensor)  # Shape: [32, 1280, 7, 7]\n",
        "print('Salida del bloque features: ', features_ex.shape)\n",
        "# Apply global average pooling, reducing the spatial dimensions to 1x1\n",
        "# it shrinks each 7x7 feature map into a single 1x1 pixel that represents the average of the original 7x7 area.\n",
        "# nn.AdaptiveAvgPool2d((2, 2)) would transform each 7x7 feature map into a 2x2 feature map.\n",
        "#This means, for each of the 1280 channels, the layer would effectively divide the 7x7 input area into four regions and calculate the average for each region. The resulting output tensor shape would be torch.Size([32, 1280, 2, 2]).\n",
        "global_avg_pool = nn.AdaptiveAvgPool2d((1, 1)) #Layer definition\n",
        "pooled_features = global_avg_pool(features_ex)  # Shape: [32, 1280, 1, 1]\n",
        "print('Salida del bloque features after avg pooling: ', pooled_features.shape )\n",
        "# Flatten the tensor\n",
        "#The 1 refers to the start_dim argument of the torch.flatten function.\n",
        "#When start_dim=1 is specified, torch.flatten will take dimensions 1, 2, and 3 (1280, 1, 1) and combine them into a single dimension. The new size of this flattened dimension will be the product of these dimensions: 1280 * 1 * 1 = 1280\n",
        "flattened_features = torch.flatten(pooled_features, 1)  # Shape: [32, 1280]\n",
        "print('Salida 1D: ', flattened_features.shape )\n",
        "\n",
        "#where is the avg pooling in the model?\n",
        "import inspect\n",
        "print(inspect.getsource(pytorch_model.forward))\n",
        "print(inspect.getsource(pytorch_model._forward_impl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ6ORDgG5ZB6"
      },
      "outputs": [],
      "source": [
        "# Sobre escribimos el clasificador de la red, el cual es la ultima capa [-1]\n",
        "pytorch_model.classifier[-1] = torch.nn.Linear(\n",
        "        in_features=1280,  # Vector de caracteristicas entregado por MobileNetV2\n",
        "        out_features=10)  # Numero de clases para CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHPlyP_tqozr"
      },
      "source": [
        "## Definición del Módulo Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEWxIedCZONS"
      },
      "outputs": [],
      "source": [
        "class Lightning_CNN(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate, classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        # Modelo PyTorch heredado\n",
        "        self.model = model\n",
        "        self.classes = classes\n",
        "\n",
        "        # Guardar hiperparametros en directorio de logs\n",
        "        # Ignora los pesos del modelo\n",
        "        self.save_hyperparameters(ignore=[\"model\"])\n",
        "\n",
        "        # Definición de métricas para cada grupo de datos\n",
        "        self.train_acc = torchmetrics.Accuracy(num_classes = self.classes, task='multiclass')\n",
        "        self.valid_acc = torchmetrics.Accuracy(num_classes = self.classes, task='multiclass')\n",
        "        self.test_acc = torchmetrics.Accuracy(num_classes = self.classes, task='multiclass')\n",
        "\n",
        "    # Defining the forward method is only necessary\n",
        "    # if you want to use a Trainer's .predict() method (optional)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    # Pasos del proceso forward comunes entre train, val, test\n",
        "    def _shared_step(self, batch):\n",
        "        features, true_labels = batch\n",
        "        logits = self(features)\n",
        "        loss = torch.nn.functional.cross_entropy(logits, true_labels) # cross entropy loss recibe logits y labels como entrada. No recibe probabilidades!\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "        predicted_labels = torch.argmax(probs, dim=1)\n",
        "\n",
        "        return loss, true_labels, predicted_labels\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.train_acc(predicted_labels, true_labels)\n",
        "        self.log(\"train_acc\", self.train_acc, on_epoch=True, on_step=False)\n",
        "\n",
        "        return loss  # this is passed to the optimzer for training\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
        "        self.log(\"valid_loss\", loss)\n",
        "        self.valid_acc(predicted_labels, true_labels)\n",
        "        self.log(\"valid_acc\", self.valid_acc, on_epoch=True, on_step=False, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
        "        self.test_acc(predicted_labels, true_labels)\n",
        "        self.log(\"test_acc\", self.test_acc, on_epoch=True, on_step=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAf9UGzLtRbw"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C86w3BYUZOIL"
      },
      "outputs": [],
      "source": [
        "# Inicialización del modulo lightning\n",
        "\n",
        "lightning_model = Lightning_CNN(pytorch_model, learning_rate=LEARNING_RATE, classes=CLASES)\n",
        "\n",
        "callback_check = ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"valid_acc\") # guardamos el mejor modelo monitoreado en la acc de validación. Por qué no la de entrenamiento?\n",
        "\n",
        "callback_tqdm = RichProgressBar(leave=True)\n",
        "\n",
        "logger = CSVLogger(save_dir=\"logs/\", name=\"complex-cnn-cifar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoN-HpDa8FJY"
      },
      "outputs": [],
      "source": [
        "class CustomLoggingCallback(pl.Callback):\n",
        "    def on_train_epoch_start(self, trainer, pl_module):\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_train_start(self, trainer, pl_module):\n",
        "        print(f\"Batch Size: {trainer.datamodule.batch_size}\")\n",
        "\n",
        "    def on_train_epoch_end(self, trainer, pl_module):\n",
        "        epoch_duration = time.time() - self.epoch_start_time\n",
        "        epoch = trainer.current_epoch\n",
        "        train_loss = trainer.callback_metrics.get(\"train_loss\")\n",
        "        valid_loss = trainer.callback_metrics.get(\"valid_loss\")\n",
        "\n",
        "        output_str = f\"Epoch {epoch}: \"\n",
        "        if train_loss is not None:\n",
        "            output_str += f\"Train Loss: {train_loss:.4f}, \"\n",
        "        if valid_loss is not None:\n",
        "            output_str += f\"Valid Loss: {valid_loss:.4f}, \"\n",
        "        output_str += f\"Time per Epoch: {epoch_duration:.2f} seconds\"\n",
        "        print(output_str)\n",
        "\n",
        "custom_logger_callback = CustomLoggingCallback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf0dXwlmZOFK"
      },
      "outputs": [],
      "source": [
        "#  Inicia entrenamiento\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=NUM_EPOCHS,\n",
        "                    callbacks=[callback_check, custom_logger_callback],\n",
        "                    accelerator=\"auto\",  # Uses GPUs or TPUs if available\n",
        "                    devices=\"auto\",  # Uses all available GPUs/TPUs if applicable\n",
        "                    logger=logger,\n",
        "                    log_every_n_steps=100,\n",
        "                    enable_progress_bar= ENABLE_PROGRESS_BAR)\n",
        "\n",
        "start_time = time.time()\n",
        "trainer.fit(model = lightning_model, datamodule = data_module)\n",
        "\n",
        "runtime = (time.time() - start_time) / 60\n",
        "print(f\"Tiempo de entrenamiento en minutos: {runtime:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aer842ctxNzM"
      },
      "source": [
        "## Graficamos las curvas de aprendizaje del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDI5akWpZOBy"
      },
      "outputs": [],
      "source": [
        "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
        "\n",
        "aggreg_metrics = []\n",
        "agg_col = \"epoch\"\n",
        "for i, dfg in metrics.groupby(agg_col):\n",
        "    agg = dict(dfg.mean())\n",
        "    agg[agg_col] = i\n",
        "    aggreg_metrics.append(agg)\n",
        "\n",
        "df_metrics = pd.DataFrame(aggreg_metrics)\n",
        "df_metrics[[\"train_loss\", \"valid_loss\"]].plot(\n",
        "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"Loss\"\n",
        ")\n",
        "df_metrics[[\"train_acc\", \"valid_acc\"]].plot(\n",
        "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"ACC\"\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4FDBGQ3xvHd"
      },
      "source": [
        "## Evaluamos el mejor modelo en el grupo de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8ZIocKjZN9a"
      },
      "outputs": [],
      "source": [
        "trainer.test(model = lightning_model, datamodule = data_module, ckpt_path = 'best') # cargamos el mejor checkpoint del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgnPlKj82EBw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pylight",
      "language": "python",
      "name": "pylight"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}